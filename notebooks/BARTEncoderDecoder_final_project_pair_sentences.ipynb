{"cells":[{"cell_type":"markdown","source":["\n","### GOAL  \n","In this notebook we are attempting to create custom encoder and decoder using BART models. For encoder we are using BARTmodel and for decoder we are using BartForConditionalGeneration model.  \n","\n","Toxic sentences have masked words in sentences will be passed to decoder.\n","The toxic sentence will be inputed to encoder.\n","\n","BART (Bidirectional and Auto-Regressive Transformers) is a pre-trained transformer-based model developed by Facebook AI Research. BART is capable of handling several natural language processing tasks such as text generation, summarization, and translation.\n","\n","BART consists of two main components, an encoder and a decoder. The encoder takes an input sequence and generates a contextualized representation of the input. The decoder takes the contextualized representation of the input generated by the encoder and generates an output sequence based on the task at hand.\n","\n","For text style transfer, the encoder-decoder architecture of BART can be used to generate output text with a different style than the input text. The encoder takes the input text and generates a contextualized representation, which is then used as input to the decoder to generate the output text in the desired style.\n","\n","The BART model has been pre-trained on a large corpus of text and can generate high-quality text in various styles, making it an ideal choice for text style transfer tasks. BARTForConditionalGeneration is a class that allows us to generate text by conditioning on specific inputs, such as the output of the encoder, making it suitable for text style transfer tasks."],"metadata":{"id":"NqzFyzA_vjSj"}},{"cell_type":"markdown","source":["### DISCLAIMER :  \n","Due to nature of experiment the notebook contains profanity and toxic words. Please proceed with caution. "],"metadata":{"id":"ms-LDKdSxQoJ"}},{"cell_type":"markdown","metadata":{"id":"so-yur1S9mS4"},"source":["## 1. Setup\n","\n","### 1.1. Libraries and Helper Functions\n","\n","This notebook requires the below prerequisites that must be downloaded. "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8uQnMctL9mS5","outputId":"d0992b57-1a63-4580-9601-654218c678e7","executionInfo":{"status":"ok","timestamp":1680460749560,"user_tz":420,"elapsed":58215,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m468.7/468.7 KB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["#@title Installs\n","!pip install transformers --quiet\n","!pip install tqdm boto3 requests regex sentencepiece sacremoses evaluate --quiet\n","!pip install rouge --quiet\n","!pip install better_profanity --quiet\n","!pip install rouge-score --quiet\n","!pip install -U evaluate --quiet"]},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"j0jokLO9GqZp"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"7CALja8vLcZ9","executionInfo":{"status":"ok","timestamp":1680461030329,"user_tz":420,"elapsed":5531,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[],"source":["import torch\n","import pandas as pd\n","from tqdm import tqdm\n","# upload external file before import\n","from google.colab import files\n","\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from transformers import DistilBertTokenizer, DistilBertModel\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from rouge import Rouge \n","from nltk.translate.bleu_score import sentence_bleu\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import time\n","import datetime\n","import string\n","import regex as re\n","from google.colab import drive"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cbELEg_S5iX","outputId":"f5ef9608-efbb-4930-a45d-dbb35017567a","executionInfo":{"status":"ok","timestamp":1680461030546,"user_tz":420,"elapsed":229,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}],"source":["# Check if a GPU is available, otherwise use CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YnqrYaGuxo3M","outputId":"c853b788-ae9c-4dde-af1e-a9d29bb86c50","executionInfo":{"status":"ok","timestamp":1680461052964,"user_tz":420,"elapsed":22424,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCThYsKtyrOn","outputId":"981f83a4-f15b-418c-c8be-53c37d8c6cbc","executionInfo":{"status":"ok","timestamp":1680243448616,"user_tz":420,"elapsed":674,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["clean_dev.csv\tclean_train.csv   original-test.tsv   test.csv\n","clean_test.csv\toriginal-dev.tsv  original-train.tsv  train.csv\n"]}],"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/data\""]},{"cell_type":"code","source":["train_path='/content/drive/MyDrive/Colab Notebooks/data/original-train.tsv'\n","dev_path='/content/drive/MyDrive/Colab Notebooks/data/original-dev.tsv'\n","test_path='/content/drive/MyDrive/Colab Notebooks/data/original-test.tsv'"],"metadata":{"id":"gMBfqnnShB06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train=pd.read_csv(train_path,sep=\"\\t\", header=0) \n","display(train[1:])"],"metadata":{"id":"7mzdBizMi1gj","colab":{"base_uri":"https://localhost:8080/","height":423},"outputId":"1f492577-6dd3-40b4-8122-41cfb4b37e8c","executionInfo":{"status":"ok","timestamp":1680243453724,"user_tz":420,"elapsed":1145,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["                                         offensive-text  \\\n","1     Ok, this makes no sense. This will create vigi...   \n","2     so fucking true. the amount of up and coming r...   \n","3     Go f yourself Republican scum who put us here ...   \n","4          Dumb fucking take. People want to do things.   \n","5                                             Fuck no ğŸ˜‚   \n","...                                                 ...   \n","1579  The View! And this crap hits my front page!? l...   \n","1580                                     Thatâ€™s racist.   \n","1581     Cultural Marxism isn't a thing you weird fuck.   \n","1582  LOL, anyone that questions the Democrat progra...   \n","1583  How about you guys actually read the bill, if ...   \n","\n","                                 style-transferred-text  \n","1     Ok, this makes no sense. This will create vigi...  \n","2     so true. the amount of up and coming rappers t...  \n","3     Republicans put us in this situation. I would ...  \n","4     That's not a smart take. People want to do thi...  \n","5                                                    no  \n","...                                                 ...  \n","1579  This must be mostly bots but still, it's stran...  \n","1580                          Those actions are racist.  \n","1581                    Cultural Marxism isn't a thing.  \n","1582  LOL, anyone that questions the Democrat progra...  \n","1583            I wish everyone actually read the bill.  \n","\n","[1583 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-97833d08-67ee-4cb7-a3c4-54da531ab2df\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>offensive-text</th>\n","      <th>style-transferred-text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>Ok, this makes no sense. This will create vigi...</td>\n","      <td>Ok, this makes no sense. This will create vigi...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>so fucking true. the amount of up and coming r...</td>\n","      <td>so true. the amount of up and coming rappers t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Go f yourself Republican scum who put us here ...</td>\n","      <td>Republicans put us in this situation. I would ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Dumb fucking take. People want to do things.</td>\n","      <td>That's not a smart take. People want to do thi...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Fuck no ğŸ˜‚</td>\n","      <td>no</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1579</th>\n","      <td>The View! And this crap hits my front page!? l...</td>\n","      <td>This must be mostly bots but still, it's stran...</td>\n","    </tr>\n","    <tr>\n","      <th>1580</th>\n","      <td>Thatâ€™s racist.</td>\n","      <td>Those actions are racist.</td>\n","    </tr>\n","    <tr>\n","      <th>1581</th>\n","      <td>Cultural Marxism isn't a thing you weird fuck.</td>\n","      <td>Cultural Marxism isn't a thing.</td>\n","    </tr>\n","    <tr>\n","      <th>1582</th>\n","      <td>LOL, anyone that questions the Democrat progra...</td>\n","      <td>LOL, anyone that questions the Democrat progra...</td>\n","    </tr>\n","    <tr>\n","      <th>1583</th>\n","      <td>How about you guys actually read the bill, if ...</td>\n","      <td>I wish everyone actually read the bill.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1583 rows Ã— 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97833d08-67ee-4cb7-a3c4-54da531ab2df')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-97833d08-67ee-4cb7-a3c4-54da531ab2df button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-97833d08-67ee-4cb7-a3c4-54da531ab2df');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niEwhBdNFnxW","outputId":"b4b931dc-20ce-4f4e-f52e-c1b7a3f543b0","executionInfo":{"status":"ok","timestamp":1680243460781,"user_tz":420,"elapsed":407,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["238"]},"metadata":{},"execution_count":8}],"source":["train['offensive-text'].str.len().max()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E2MqDpHEGNyi","outputId":"da8280e4-494f-453c-ad61-66cd4bd2691b","executionInfo":{"status":"ok","timestamp":1680243463365,"user_tz":420,"elapsed":811,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":9}],"source":["train['offensive-text'].str.len().min()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pBfPJTp0Tg6T","outputId":"1d8127e5-8300-4ec2-bca5-5fe456575643","executionInfo":{"status":"ok","timestamp":1680243464574,"user_tz":420,"elapsed":12,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["69.85353535353535"]},"metadata":{},"execution_count":10}],"source":["train['offensive-text'].str.len().mean()"]},{"cell_type":"code","source":["train[train['style-transferred-text']=='']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"id":"i8Qz4GMUDCV3","outputId":"865a358d-b4d2-4d7f-9f1b-0db9bd57c0e5","executionInfo":{"status":"ok","timestamp":1680243472578,"user_tz":420,"elapsed":274,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [offensive-text, style-transferred-text]\n","Index: []"],"text/html":["\n","  <div id=\"df-681b2c4f-c26a-466e-b24f-2e8db3fc563b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>offensive-text</th>\n","      <th>style-transferred-text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-681b2c4f-c26a-466e-b24f-2e8db3fc563b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-681b2c4f-c26a-466e-b24f-2e8db3fc563b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-681b2c4f-c26a-466e-b24f-2e8db3fc563b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["dev=pd.read_csv(dev_path,sep=\"\\t\", header=0) \n","display(dev)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"rJ-0TJFCChu5","outputId":"7014b706-2942-4733-b42f-5a9413ffe4df","executionInfo":{"status":"ok","timestamp":1680243488522,"user_tz":420,"elapsed":911,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["                                        offensive-text  \\\n","0    Anyone who thinks canceling a pipeline and han...   \n","1    My favorite part is that youâ€™re still madly re...   \n","2    But did you try it? Lol. That and medical mari...   \n","3    By the downvotes, I see there are a bunch of u...   \n","4                         CUCKOLD Carlson is a problem   \n","..                                                 ...   \n","193                             Karma whore right here   \n","194  Anyone who is still huffing paint thinner by c...   \n","195         And you have never made a stupid decision?   \n","196  You couldn't maybe just think about this for a...   \n","197  Where the fuck are the mods? Seriously this is...   \n","\n","                                style-transferred-text  \n","0    Anyone who thinks canceling a pipeline and han...  \n","1     My favorite part is that you are still replying.  \n","2    Have you tried it? That helped my cousin's tin...  \n","3    By the downvotes, I see there are a bunch of T...  \n","4                               Carlson is the problem  \n","..                                                 ...  \n","193                       Hunting for karma right here  \n","194  Anyone who is still huffing paint thinner by c...  \n","195          And you have never made a wrong decision?  \n","196  You couldn't maybe just come up with the answe...  \n","197  Where are the mods? Seriously this is the most...  \n","\n","[198 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-1a74bdd7-5b44-45bc-afb9-b994c3d05102\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>offensive-text</th>\n","      <th>style-transferred-text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Anyone who thinks canceling a pipeline and han...</td>\n","      <td>Anyone who thinks canceling a pipeline and han...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>My favorite part is that youâ€™re still madly re...</td>\n","      <td>My favorite part is that you are still replying.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>But did you try it? Lol. That and medical mari...</td>\n","      <td>Have you tried it? That helped my cousin's tin...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>By the downvotes, I see there are a bunch of u...</td>\n","      <td>By the downvotes, I see there are a bunch of T...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CUCKOLD Carlson is a problem</td>\n","      <td>Carlson is the problem</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>193</th>\n","      <td>Karma whore right here</td>\n","      <td>Hunting for karma right here</td>\n","    </tr>\n","    <tr>\n","      <th>194</th>\n","      <td>Anyone who is still huffing paint thinner by c...</td>\n","      <td>Anyone who is still huffing paint thinner by c...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>And you have never made a stupid decision?</td>\n","      <td>And you have never made a wrong decision?</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>You couldn't maybe just think about this for a...</td>\n","      <td>You couldn't maybe just come up with the answe...</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>Where the fuck are the mods? Seriously this is...</td>\n","      <td>Where are the mods? Seriously this is the most...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>198 rows Ã— 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a74bdd7-5b44-45bc-afb9-b994c3d05102')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1a74bdd7-5b44-45bc-afb9-b994c3d05102 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1a74bdd7-5b44-45bc-afb9-b994c3d05102');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["def read_process_data(df):\n","    # SPECIAL_CHARS_PATTERN = re.compile(r\"(\\*)|(\\=\\=)|(\\~)|(\\=)|(\\.\\.\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\|)|()|()|(\\%)|($)|(\\>)|(\\<)|(\\{)|(\\})\")\n","    df[\"offensive-text\"] = df[\"offensive-text\"].str.lower()\n","    df[\"offensive-text\"] = df[\"offensive-text\"].str.replace(rf\"([{string.punctuation}])+\",\" \", regex=True)\n","\n","    df[\"offensive-text\"] = df[\"offensive-text\"].str.replace(\"\\xa0\", \" \", regex=False).str.split().str.join(\" \") #\\n|\\r|\\r\\n|\n","\n","    df[\"style-transferred-text\"] = df[\"style-transferred-text\"].str.lower()\n","    df[\"style-transferred-text\"] = df[\"style-transferred-text\"].str.replace(rf\"([{string.punctuation}])+\",\" \", regex=True)\n","    df[\"style-transferred-text\"] = df[\"style-transferred-text\"].str.replace(\"\\xa0\", \" \", regex=False).str.split().str.join(\" \")\n","  \n","    df['offensive-text']=df['offensive-text'].astype(str).apply(lambda x: x.encode('latin-1', 'ignore').decode('latin-1'))\n","    df[\"style-transferred-text\"] = df[\"style-transferred-text\"].astype(str).apply(lambda x: x.encode('latin-1', 'ignore').decode('latin-1'))\n","\n","    print('The null count :: ',df.isnull().sum())\n","    return df"],"metadata":{"id":"5d13W9myt0w3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PASS PROCESS \"True\" TO CREATE CLEANED DATASET : ONE TIME EFFORT\n","def cleaned_data(process=False):\n","  if process:\n","    x=read_process_data(train)\n","    x.to_csv('/content/drive/MyDrive/Colab Notebooks/data/clean_train.csv', header = False)\n","    z=read_process_data(test)\n","    z.to_csv('/content/drive/MyDrive/Colab Notebooks/data/clean_test.csv', header = False)\n","    y=read_process_data(dev)\n","    y.to_csv('/content/drive/MyDrive/Colab Notebooks/data/clean_dev.csv', header = False)\n","\n","cleaned_data(process=False)"],"metadata":{"id":"h1sOu0-tufhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_path='/content/drive/MyDrive/Colab Notebooks/data/clean_train.csv'\n","dev_path='/content/drive/MyDrive/Colab Notebooks/data/clean_dev.csv'\n","test_path='/content/drive/MyDrive/Colab Notebooks/data/clean_test.csv'\n","\n","def format_time(seconds):\n","    return str(datetime.timedelta(seconds=int(round(seconds))))"],"metadata":{"id":"94gfhdG80nt5","executionInfo":{"status":"ok","timestamp":1680461061965,"user_tz":420,"elapsed":236,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#uncomment if memory issue happens. This will free up some space\n","# import torch\n","# torch.cuda.empty_cache()\n","BADWORDS = [\n","    'suck',\n","    'stupid',\n","    'pimp',\n","    'dumb',\n","    'homo',\n","    'slut',\n","    'damn',\n","    'ass',\n","    'rape',\n","    'poop',\n","    'cock',\n","    'lol',\n","    'crap',\n","    'sex',\n","    'nazi',\n","    'neo-nazi',\n","    'fuck',\n","    'bitch',\n","    'pussy',\n","    'penis',\n","    'vagina',\n","    'whore',\n","    'shit',\n","    'nigger',\n","    'nigga',\n","    'cocksucker',\n","    'assrape',\n","    'motherfucker',\n","    'wanker',\n","    'cunt',\n","    'faggot',\n","    'fags',\n","    'asshole',\n","    'piss',\n","    'cum',\n","    'moron',\n","    'cuckold',\n","    'shit' ,\n","    'filthy','retarded','screw','cucked'\n","]"],"metadata":{"id":"I-Ck9Yi_uCL9","executionInfo":{"status":"ok","timestamp":1680461081430,"user_tz":420,"elapsed":217,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oeRVnvQ10d1K"},"source":["### CUSTOM ENCODER DECODER STRUCTURE USING BART MODELS\n","Below code is implementation of BART (Bidirectional and Auto-Regressive Transformer) model for sequence-to-sequence (seq2seq) tasks using PyTorch. The BART model is a variant of the Transformer model and is pre-trained on a large corpus of text data using a denoising autoencoder objective. It is capable of generating high-quality text with coherent sentence structures.\n","\n","The code defines a PyTorch Dataset class MyDataset that reads the input and target text sequences from a text file and tokenizes them using the BART tokenizer. It then defines the collate_fn function to pad the sequences in each batch to the same length. The code then defines the BartEncoderDecoder class, which is the main model class that uses the BART encoder and decoder architecture.\n","\n","The code instantiates the BART tokenizer and the BartEncoderDecoder model, and loads the training and validation data using the MyDataset class. It also defines the optimizer and loss function. The code then runs the training loop for a specified number of epochs, during which it trains the model on the training set and evaluates it on the validation set.\n","\n","During training, the code iterates over the batches of training data and performs forward and backward passes through the model to compute the loss and update the model parameters using the Adam optimizer. The code also implements gradient accumulation by accumulating the gradients over a specified number of batches to reduce the memory requirements during training. After training on all the batches, the code computes the average loss over the entire training set and evaluates the model on the validation set.\n","\n","During validation, the code iterates over the batches of validation data and generates the predicted text sequences using the generate method of the BART decoder. It then computes the loss between the predicted and target sequences and accumulates the loss over all the batches to compute the average loss over the entire validation set. Finally, the code prints the average training and validation loss for each epoch."]},{"cell_type":"markdown","source":["### Few Choices made during development\n","\n","1. Used tokenizer = \"facebook/bart-base\" based on available resources\n","\n","2. To mask profanity words used better_profanity python package and along with this used BADWORD custom list to mask toxic words.\n","\n","3. Limited Batch size to 16 as training dataset has 1584 records. Also used low number of epochs. All hyperparameters are set based on available resources.\n","\n","4. We had a choice to use decoder.generate() or decoder.outputs() to get generated token ids. We will be using generate method. torch.argmax(outputs.logits, dim=-1) gives the index of the highest logit value for each position in the output sequence. This is not the same as the sequence of generated IDs produced by decoder.generate(), because decoder.generate() applies several additional steps during decoding, such as beam search and top-k sampling, that are not present in a simple argmax operation.\n","The decoder.generate() method in BART uses beam search to generate multiple possible sequences, and then returns the highest-scoring sequence according to a scoring function that takes into account both the log-likelihood of the sequence and various penalties for things like repeated n-grams. This process is more complex than simply taking the argmax of the logits, and can lead to better quality generated sequences."],"metadata":{"id":"ix75loEIyEDe"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"6olz1mhT-ovf","executionInfo":{"status":"ok","timestamp":1680461075785,"user_tz":420,"elapsed":1067,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["247479d6915b4eb5946d76318777ef06","ffd2eeb7c0b44f9c9aae80a32998aa5f","5246864ab21f46cfa41ac9967dde4781","dd3f8462e0be453aa909a0da2a6f4ab4","6b194fa74ec246c8af12d7d2037ce685","2250b1f8de13464badc9818447ab44cd","43b4da794d5c4b78ab15a4e1d9df6858","05b93dc3d8d146eda44c3c583c4b8688","9ca611a654244484aea941e3bb7dd8f4","9f4ccdfdd3d3440fa7280efe61caf1f1","c2c62bdf0e634af7bf4eaa0685cee31d","f1bb2c2fa2694147a86259cbf3b8e6f5","1e731dbab8ec47de9cc6dae743299a05","f0f8750592604a10af0e0037a9306f6d","91fcf5c26b434c468def649b20ec4587","99e24f93bbdd43d682bb5538988a9458","84489ba1107947f4aad96be3bf7e34a6","de8a620064694039844226e4693a974f","b45da579ab114b2f88a90812f306438d","2e9bbfbdc39d423d8b67eb64c308a7d3","49bd0896a5b443b1b6a1a4402bab4a29","2d47cdcfa2c94fec940a4490cf2b901d","5bbc0db1ff204b3ca0b7fd8ec9f03eb0","8e91a4adf2da4434ac9c33eb0f18b1ac","f2704b01fe424387ad8610ad32e2ac45","1c251e5da0654cbb88c23a2b50fc4bd9","fbc620e9ff9042dfbc7f8b72c3a4ee52","a2e1ee941c8549afa4b69312026840a8","659b824bc08842779f2eb6593bdd4b1f","a6155c0592924322a10a06506e202cd5","ff361219466f4f0a803f3ab452d2226e","0e6c6524c5704ba8bdee65bcd4c28a7c","88cafc20b5b94535a1d333a1ad5ddc99"]},"outputId":"3836f1a7-a155-4822-84f0-e1045fa6b293"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"247479d6915b4eb5946d76318777ef06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1bb2c2fa2694147a86259cbf3b8e6f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bbc0db1ff204b3ca0b7fd8ec9f03eb0"}},"metadata":{}}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import Adam\n","from transformers import BartTokenizer, BartConfig, BartForConditionalGeneration,BartModel,AutoTokenizer\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from better_profanity import profanity\n","\n","# Instantiate BART tokenizer\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-base') \n","profanity.load_censor_words()\n","\n","class MyDataset(Dataset):\n","    \"\"\"\n","    this class generate tokens of input, masked and target sentences \n","    data_path: The path to the training data file.\n","    max_seq_len: The maximum sequence length allowed for the input sequences.\n","    kind : contains value like training, validation etc . This is used to print the token generation of phases like training , validation or testing\n","\n","    In the __init__ method, the training data is loaded from the specified file, and each line of text is stored in a list.\n","    In the __len__ method, the length of the dataset is returned (i.e., the number of lines in the training data file).\n","    In the __getitem__ method, an individual training example is retrieved based on the specified index. \n","                              The input text is tokenized using the specified tokenizer and truncated to the specified maximum sequence length, \n","                              and the resulting token ids are converted to a PyTorch tensor and returned as the training example.\n","    \"\"\"\n","    def __init__(self, data_path, max_seq_len,kind):\n","        \n","        self.max_seq_len = max_seq_len\n","\n","        self.toxic_ids = []\n","        self.non_toxic_ids = []\n","        self.masked_toxic_ids=[]\n","\n","        c=0\n","        with open(data_path, 'r') as f:\n","            for line in f:\n","                line = line.strip()\n","                if line:\n","                    parts = line.split(',')\n","                    toxic_seq = parts[1]\n","                    non_toxic_seq = parts[2]\n","\n","                    censored_text = profanity.censor(toxic_seq)\n","                    masked_toxic_seq=censored_text.replace('****','<mask>')\n","\n","                    for words in BADWORDS:\n","                      masked_toxic_seq=masked_toxic_seq.replace(words,'<mask>')\n","               \n","                    toxic_ids = tokenizer(toxic_seq, max_length=max_seq_len,padding=\"max_length\", truncation=True)  \n","                    non_toxic_ids = tokenizer(non_toxic_seq, max_length=max_seq_len,padding=\"max_length\", truncation=True)\n","                    masked_toxic_ids= tokenizer(masked_toxic_seq, max_length=max_seq_len,padding=\"max_length\", truncation=True)\n","                    \n","                    self.toxic_ids.append(toxic_ids.input_ids)\n","                    self.non_toxic_ids.append(non_toxic_ids.input_ids)\n","                    self.masked_toxic_ids.append(masked_toxic_ids.input_ids)\n","\n","                    if c==0:\n","                      print('\\n',kind,' dataset preview .....')\n","                      print('*******************************************')\n","                      print('Input :: toxic        ::',toxic_seq)\n","                      print('Input :: masked toxic ::',masked_toxic_seq)\n","                      print('Input :: non toxic    ::',non_toxic_seq)\n","                      print('encoded length toxic :',toxic_ids)\n","                      print('encoded length masked toxic: ',masked_toxic_ids)\n","                      print('encoded length non toxic: ',non_toxic_ids)\n","                      print('*******************************************')\n","                    c=c+1\n","            print('total number of',kind,' data processed : ',c)\n","\n","    def __len__(self):\n","        return len(self.toxic_ids)\n","\n","    def __getitem__(self, index):\n","        toxic_ids = self.toxic_ids[index]\n","        non_toxic_ids = self.non_toxic_ids[index]\n","        masked_toxic_ids =self.masked_toxic_ids[index]\n","\n","        # Create a PyTorch tensor from the tokenized sequences\n","        toxic_ids = torch.tensor(toxic_ids, dtype=torch.int64)\n","        non_toxic_ids = torch.tensor(non_toxic_ids, dtype=torch.int64)\n","        masked_toxic_ids = torch.tensor(masked_toxic_ids, dtype=torch.int64)\n","\n","        return {'toxic_ids': toxic_ids, 'non_toxic_ids': non_toxic_ids, 'masked_toxic_ids': masked_toxic_ids}\n"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"TYQd-fO70q4z","executionInfo":{"status":"ok","timestamp":1680463587002,"user_tz":420,"elapsed":7914,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"outputs":[],"source":["# Define hyperparameters\n","\n","BATCH_SIZE = 16 \n","NUM_EPOCHS = 4\n","LEARNING_RATE = 2e-5 \n","WEIGHT_DECAY = 2e-5\n","MAX_SEQ_LEN = 128 \n","D_MODEL = 256 #dimensionality of the input and output vectors of the Transformer model. It is also commonly referred to as the \"hidden size\" of the model.POWER OF 2\n","NUM_HEADS = 4 #number of attention heads used in multi-head attention (D_MODEL/NUM_HEADS=INT)\n","NUM_ENCODER_LAYERS = 4 \n","NUM_DECODER_LAYERS = 4 \n","DIM_FEEDFORWARD = 1024 #(4 times of D_MODEL)\n","DROPOUT = 0.3\n","GRADIENT_ACCUMULATION_STEPS = 2 # accumulate gradients over 2 batches\n","HIDDEN_SIZE = 768\n","\n"," # Define BART encoder-decoder model\n","class BartEncoderDecoder(nn.Module):\n","    def __init__(self, bart_config):\n","        super().__init__()\n","\n","        self.encoder = BartModel.from_pretrained('facebook/bart-base') #The bare BART Model outputting raw hidden-states without any specific head on top.\n","        self.encoder.config = bart_config\n","        self.decoder = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n","        self.decoder.config = bart_config\n","        self.bart_config=bart_config\n","       \n","    def forward(self, input_ids, masked_input):\n","\n","        # Generate hidden representations for the modified input sequence using the encoder\n","        encoder_outputs = self.encoder(input_ids=input_ids)\n","\n","        # Pass the modified input sequence as input to the decoder and generate the output sequence\n","        decoder_input_ids = masked_input\n","        decoder_outputs = self.decoder(input_ids=decoder_input_ids, encoder_outputs=encoder_outputs.encoder_last_hidden_state)\n","        \n","        return decoder_outputs\n","\n","# Instantiate BART encoder-decoder model\n","bart_config = BartConfig(d_model=D_MODEL, encoder_layers=NUM_ENCODER_LAYERS, decoder_layers=NUM_DECODER_LAYERS, \n","                         encoder_attention_heads=NUM_HEADS, decoder_attention_heads=NUM_HEADS,\n","                         encoder_ffn_dim=DIM_FEEDFORWARD, decoder_ffn_dim=DIM_FEEDFORWARD, dropout=DROPOUT, hidden_size=HIDDEN_SIZE) #\n","bart_model = BartEncoderDecoder(bart_config).cuda() # move the model to CUDA"]},{"cell_type":"code","source":["def loss_fn(outputs, targets):\n","    batch_size, seq_len, vocab_size = outputs.logits.shape\n","    outputs = outputs.logits.view(batch_size * seq_len, vocab_size)\n","    \n","    # Reshape the targets tensor to (batch_size * seq_len)\n","    targets = targets.view(-1)\n","    loss = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)(outputs, targets)\n","    return loss\n","\n","def collate_fn(batch, batch_size=BATCH_SIZE):\n","    # Pad sequences in batch to have the same length\n","    toxic_ids = pad_sequence([item['toxic_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n","    masked_toxic_ids = pad_sequence([item['masked_toxic_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n","    non_toxic_ids = pad_sequence([item['non_toxic_ids'] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    # Split data into batches of the desired size\n","    num_batches = len(batch) // batch_size\n","    if len(batch) % batch_size != 0:\n","        num_batches += 1\n","    toxic_batches = list(torch.split(toxic_ids[:num_batches*batch_size], batch_size))\n","    masked_toxic_batches = list(torch.split(masked_toxic_ids[:num_batches*batch_size], batch_size))\n","    non_toxic_batches = list(torch.split(non_toxic_ids[:num_batches*batch_size], batch_size))\n","\n","    # Pad the last batch to ensure that all batches have the same size\n","    if len(toxic_batches[-1]) < batch_size:\n","        toxic_batches[-1] = nn.functional.pad(toxic_batches[-1], (0, 0, 0, batch_size - len(toxic_batches[-1])), value=tokenizer.pad_token_id)\n","        masked_toxic_batches[-1] = nn.functional.pad(masked_toxic_batches[-1], (0, 0, 0, batch_size - len(masked_toxic_batches[-1])), value=tokenizer.pad_token_id)\n","        non_toxic_batches[-1] = nn.functional.pad(non_toxic_batches[-1], (0, 0, 0, batch_size - len(non_toxic_batches[-1])), value=tokenizer.pad_token_id)\n","\n","    return [{'toxic_ids': t, 'non_toxic_ids': nt,'masked_toxic_ids': mt} for t, nt, mt in zip(toxic_batches, non_toxic_batches,masked_toxic_batches)][0]"],"metadata":{"id":"r2hZfkqE9c2I","executionInfo":{"status":"ok","timestamp":1680461166768,"user_tz":420,"elapsed":162,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["bart_model"],"metadata":{"id":"2T65jPqJVe3H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5d874eb1-e368-4056-f371-0bc382702f03","executionInfo":{"status":"ok","timestamp":1680463597515,"user_tz":420,"elapsed":16,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BartEncoderDecoder(\n","  (encoder): BartModel(\n","    (shared): Embedding(50265, 768, padding_idx=1)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","      (layers): ModuleList(\n","        (0): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","      (layers): ModuleList(\n","        (0): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (decoder): BartForConditionalGeneration(\n","    (model): BartModel(\n","      (shared): Embedding(50265, 768, padding_idx=1)\n","      (encoder): BartEncoder(\n","        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","        (layers): ModuleList(\n","          (0): BartEncoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (activation_fn): GELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (1): BartEncoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (activation_fn): GELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (2): BartEncoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (activation_fn): GELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (3): BartEncoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (activation_fn): GELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (4): BartEncoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (activation_fn): GELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (5): BartEncoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (activation_fn): GELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (decoder): BartDecoder(\n","        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","        (layers): ModuleList(\n","          (0): BartDecoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (activation_fn): GELUActivation()\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (1): BartDecoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (activation_fn): GELUActivation()\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (2): BartDecoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (activation_fn): GELUActivation()\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (3): BartDecoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (activation_fn): GELUActivation()\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (4): BartDecoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (activation_fn): GELUActivation()\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (5): BartDecoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (activation_fn): GELUActivation()\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n","  )\n",")"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["import torch.autograd as autograd\n","# CUDA_LAUNCH_BLOCKING=1\n","\n","# Load training and validation data\n","train_data = MyDataset(train_path, MAX_SEQ_LEN,'training')\n","val_data = MyDataset(dev_path, MAX_SEQ_LEN,'validation')\n","\n","print('Instatiating Data loaders for training and validation dataset ......')\n","# Instantiate data loaders\n","train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_fn)\n","val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False,collate_fn=collate_fn)\n","\n","def train(train_loader,learning_rate=LEARNING_RATE, epochs=NUM_EPOCHS, verbose=True, batch_size=BATCH_SIZE):\n","  autograd.set_detect_anomaly(True)\n","\n","  # Instantiate optimizer and loss function\n","  optimizer = Adam(bart_model.parameters(), lr=learning_rate) #, weight_decay=WEIGHT_DECAY\n","  train_losses=[]\n","  \n","  print('\\nTokenizer information :: ')\n","  print('eos_token_id :',tokenizer.eos_token_id)\n","  print('pad_token_id :',tokenizer.pad_token_id)\n","  print('bos_token_id :', tokenizer.bos_token_id)\n","  print('sep_token_id :', tokenizer.sep_token_id,'\\n')\n","\n","  # start time\n","  t0 = time.time()\n","\n","  # Training loop\n","  for epoch in range(epochs):\n","      print ('######  Epoch {}/{} ######'.format(epoch+1, epochs))\n","      \n","      # Train the model\n","      bart_model.train()\n","      train_loss = 0\n","      epoch_loss=0\n","      for step, batch in enumerate(train_loader):\n","          src = batch['toxic_ids'].cuda()\n","          tgt = batch['non_toxic_ids'].cuda()\n","          masked_src=batch['masked_toxic_ids'].cuda()\n","\n","          optimizer.zero_grad()\n","          outputs=bart_model(input_ids=src, masked_input=masked_src)\n","\n","          loss = loss_fn(outputs, tgt.cuda())\n","          loss = loss.mean() # reduce the loss tensor to a scalar tensor by taking the mean\n","          loss /= GRADIENT_ACCUMULATION_STEPS\n","\n","          loss.backward()\n","          if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","          epoch_loss += loss.item()\n","\n","          # Report progress every batch\n","          if verbose and step % 10 == 0:\n","              print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, step+1, len(train_loader), loss.item()))\n","\n","      avg_loss = epoch_loss / len(train_loader)\n","      train_losses.append(avg_loss)\n","\n","      elapsed_time = time.time() - t0\n","      if verbose:\n","          print('Epoch [{}/{}], Average Loss: {:.4f}, Elapsed Time: {}'.format(epoch+1, epochs, avg_loss, format_time(elapsed_time)))\n","\n","  return train_losses, sum(train_losses) / len(train_losses)\n","\n","train_losses,avg_loss=train(train_loader)"],"metadata":{"id":"kNySrGWAVZM5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"09af9821-b59a-4d6b-e09e-e0349e61d84c","executionInfo":{"status":"ok","timestamp":1680464101665,"user_tz":420,"elapsed":493692,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," training  dataset preview .....\n","*******************************************\n","Input :: toxic        :: pussy nobody asked for your input\n","Input :: masked toxic :: <mask> nobody asked for your input\n","Input :: non toxic    :: nobody asked for your input\n","encoded length toxic : {'input_ids': [0, 642, 28102, 5907, 553, 13, 110, 8135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","encoded length masked toxic:  {'input_ids': [0, 50264, 5907, 553, 13, 110, 8135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","encoded length non toxic:  {'input_ids': [0, 33212, 9956, 553, 13, 110, 8135, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","*******************************************\n","total number of training  data processed :  1584\n","\n"," validation  dataset preview .....\n","*******************************************\n","Input :: toxic        :: anyone who thinks canceling a pipeline and handcuffing fossil energy production in the us would have no effect on gas prices is a fucking moron\n","Input :: masked toxic :: anyone who thinks canceling a pipeline and handcuffing fossil energy production in the us would have no effect on gas prices is a <mask> <mask>\n","Input :: non toxic    :: anyone who thinks canceling a pipeline and handcuffing fossil energy production in the us would have no effect on gas prices is not thinking about this\n","encoded length toxic : {'input_ids': [0, 3785, 1264, 54, 4265, 21338, 1527, 10, 4116, 8, 42149, 5865, 154, 11422, 1007, 931, 11, 5, 201, 74, 33, 117, 1683, 15, 1123, 850, 16, 10, 23523, 14628, 261, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","encoded length masked toxic:  {'input_ids': [0, 3785, 1264, 54, 4265, 21338, 1527, 10, 4116, 8, 42149, 5865, 154, 11422, 1007, 931, 11, 5, 201, 74, 33, 117, 1683, 15, 1123, 850, 16, 10, 50264, 50264, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","encoded length non toxic:  {'input_ids': [0, 3785, 1264, 54, 4265, 21338, 1527, 10, 4116, 8, 42149, 5865, 154, 11422, 1007, 931, 11, 5, 201, 74, 33, 117, 1683, 15, 1123, 850, 16, 45, 2053, 59, 42, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","*******************************************\n","total number of validation  data processed :  198\n","Instatiating Data loaders for training and validation dataset ......\n","\n","Tokenizer information :: \n","eos_token_id : 2\n","pad_token_id : 1\n","bos_token_id : 0\n","sep_token_id : 2 \n","\n","######  Epoch 1/4 ######\n","Epoch [1/4], Step [1/99], Loss: 5.6973\n","Epoch [1/4], Step [11/99], Loss: 3.3877\n","Epoch [1/4], Step [21/99], Loss: 3.1459\n","Epoch [1/4], Step [31/99], Loss: 2.9574\n","Epoch [1/4], Step [41/99], Loss: 3.0392\n","Epoch [1/4], Step [51/99], Loss: 3.0892\n","Epoch [1/4], Step [61/99], Loss: 2.9326\n","Epoch [1/4], Step [71/99], Loss: 2.6796\n","Epoch [1/4], Step [81/99], Loss: 3.0088\n","Epoch [1/4], Step [91/99], Loss: 2.7393\n","Epoch [1/4], Average Loss: 3.1172, Elapsed Time: 0:01:44\n","######  Epoch 2/4 ######\n","Epoch [2/4], Step [1/99], Loss: 2.8374\n","Epoch [2/4], Step [11/99], Loss: 2.9849\n","Epoch [2/4], Step [21/99], Loss: 2.8974\n","Epoch [2/4], Step [31/99], Loss: 2.6530\n","Epoch [2/4], Step [41/99], Loss: 2.7235\n","Epoch [2/4], Step [51/99], Loss: 2.7075\n","Epoch [2/4], Step [61/99], Loss: 2.8661\n","Epoch [2/4], Step [71/99], Loss: 3.0942\n","Epoch [2/4], Step [81/99], Loss: 2.8526\n","Epoch [2/4], Step [91/99], Loss: 2.6105\n","Epoch [2/4], Average Loss: 2.7795, Elapsed Time: 0:03:26\n","######  Epoch 3/4 ######\n","Epoch [3/4], Step [1/99], Loss: 2.6189\n","Epoch [3/4], Step [11/99], Loss: 2.7563\n","Epoch [3/4], Step [21/99], Loss: 2.6380\n","Epoch [3/4], Step [31/99], Loss: 2.6049\n","Epoch [3/4], Step [41/99], Loss: 2.7148\n","Epoch [3/4], Step [51/99], Loss: 2.7953\n","Epoch [3/4], Step [61/99], Loss: 2.7306\n","Epoch [3/4], Step [71/99], Loss: 2.6328\n","Epoch [3/4], Step [81/99], Loss: 2.8293\n","Epoch [3/4], Step [91/99], Loss: 2.7904\n","Epoch [3/4], Average Loss: 2.6964, Elapsed Time: 0:05:10\n","######  Epoch 4/4 ######\n","Epoch [4/4], Step [1/99], Loss: 2.8056\n","Epoch [4/4], Step [11/99], Loss: 2.9687\n","Epoch [4/4], Step [21/99], Loss: 2.5410\n","Epoch [4/4], Step [31/99], Loss: 2.5854\n","Epoch [4/4], Step [41/99], Loss: 2.7805\n","Epoch [4/4], Step [51/99], Loss: 2.4400\n","Epoch [4/4], Step [61/99], Loss: 2.5448\n","Epoch [4/4], Step [71/99], Loss: 2.7603\n","Epoch [4/4], Step [81/99], Loss: 2.5561\n","Epoch [4/4], Step [91/99], Loss: 2.6530\n","Epoch [4/4], Average Loss: 2.6364, Elapsed Time: 0:06:55\n"]}]},{"cell_type":"code","source":["#uncomment below one only in case of cuda memory issue\n","# import torch\n","# torch.cuda.empty_cache()"],"metadata":{"id":"7Zm3RLmyvxbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_losses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQfgit4BLfue","outputId":"e2c76ab6-16ab-41d7-b3b8-896e5db7b517","executionInfo":{"status":"ok","timestamp":1680464135307,"user_tz":420,"elapsed":8,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3.117163983258334, 2.779476264510492, 2.6963960666849154, 2.636428228532425]"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["\n","torch.save(bart_model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/models/bart_model_checkpoint.pth')\n","# download checkpoint file\n","files.download('/content/drive/MyDrive/Colab Notebooks/models/bart_model_checkpoint.pth')\n","# bart_model = bart_model.cuda()"],"metadata":{"id":"vIsZ-Es7qyOq","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cb61644d-9e05-487a-eb9d-a62370eb90cf","executionInfo":{"status":"ok","timestamp":1680464141051,"user_tz":420,"elapsed":4588,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_5b902008-cb8d-4e1c-ac6f-7d81cdccaa08\", \"bart_model_checkpoint.pth\", 1115771505)"]},"metadata":{}}]},{"cell_type":"code","source":["state_dict = torch.load('/content/drive/MyDrive/Colab Notebooks/models/bart_model_checkpoint.pth')\n","print(state_dict.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTabPdBs7_qc","outputId":"4aa0c32a-58db-4bcd-e8b4-2d8420d809b1","executionInfo":{"status":"ok","timestamp":1680464142931,"user_tz":420,"elapsed":1888,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["odict_keys(['encoder.shared.weight', 'encoder.encoder.embed_tokens.weight', 'encoder.encoder.embed_positions.weight', 'encoder.encoder.layers.0.self_attn.k_proj.weight', 'encoder.encoder.layers.0.self_attn.k_proj.bias', 'encoder.encoder.layers.0.self_attn.v_proj.weight', 'encoder.encoder.layers.0.self_attn.v_proj.bias', 'encoder.encoder.layers.0.self_attn.q_proj.weight', 'encoder.encoder.layers.0.self_attn.q_proj.bias', 'encoder.encoder.layers.0.self_attn.out_proj.weight', 'encoder.encoder.layers.0.self_attn.out_proj.bias', 'encoder.encoder.layers.0.self_attn_layer_norm.weight', 'encoder.encoder.layers.0.self_attn_layer_norm.bias', 'encoder.encoder.layers.0.fc1.weight', 'encoder.encoder.layers.0.fc1.bias', 'encoder.encoder.layers.0.fc2.weight', 'encoder.encoder.layers.0.fc2.bias', 'encoder.encoder.layers.0.final_layer_norm.weight', 'encoder.encoder.layers.0.final_layer_norm.bias', 'encoder.encoder.layers.1.self_attn.k_proj.weight', 'encoder.encoder.layers.1.self_attn.k_proj.bias', 'encoder.encoder.layers.1.self_attn.v_proj.weight', 'encoder.encoder.layers.1.self_attn.v_proj.bias', 'encoder.encoder.layers.1.self_attn.q_proj.weight', 'encoder.encoder.layers.1.self_attn.q_proj.bias', 'encoder.encoder.layers.1.self_attn.out_proj.weight', 'encoder.encoder.layers.1.self_attn.out_proj.bias', 'encoder.encoder.layers.1.self_attn_layer_norm.weight', 'encoder.encoder.layers.1.self_attn_layer_norm.bias', 'encoder.encoder.layers.1.fc1.weight', 'encoder.encoder.layers.1.fc1.bias', 'encoder.encoder.layers.1.fc2.weight', 'encoder.encoder.layers.1.fc2.bias', 'encoder.encoder.layers.1.final_layer_norm.weight', 'encoder.encoder.layers.1.final_layer_norm.bias', 'encoder.encoder.layers.2.self_attn.k_proj.weight', 'encoder.encoder.layers.2.self_attn.k_proj.bias', 'encoder.encoder.layers.2.self_attn.v_proj.weight', 'encoder.encoder.layers.2.self_attn.v_proj.bias', 'encoder.encoder.layers.2.self_attn.q_proj.weight', 'encoder.encoder.layers.2.self_attn.q_proj.bias', 'encoder.encoder.layers.2.self_attn.out_proj.weight', 'encoder.encoder.layers.2.self_attn.out_proj.bias', 'encoder.encoder.layers.2.self_attn_layer_norm.weight', 'encoder.encoder.layers.2.self_attn_layer_norm.bias', 'encoder.encoder.layers.2.fc1.weight', 'encoder.encoder.layers.2.fc1.bias', 'encoder.encoder.layers.2.fc2.weight', 'encoder.encoder.layers.2.fc2.bias', 'encoder.encoder.layers.2.final_layer_norm.weight', 'encoder.encoder.layers.2.final_layer_norm.bias', 'encoder.encoder.layers.3.self_attn.k_proj.weight', 'encoder.encoder.layers.3.self_attn.k_proj.bias', 'encoder.encoder.layers.3.self_attn.v_proj.weight', 'encoder.encoder.layers.3.self_attn.v_proj.bias', 'encoder.encoder.layers.3.self_attn.q_proj.weight', 'encoder.encoder.layers.3.self_attn.q_proj.bias', 'encoder.encoder.layers.3.self_attn.out_proj.weight', 'encoder.encoder.layers.3.self_attn.out_proj.bias', 'encoder.encoder.layers.3.self_attn_layer_norm.weight', 'encoder.encoder.layers.3.self_attn_layer_norm.bias', 'encoder.encoder.layers.3.fc1.weight', 'encoder.encoder.layers.3.fc1.bias', 'encoder.encoder.layers.3.fc2.weight', 'encoder.encoder.layers.3.fc2.bias', 'encoder.encoder.layers.3.final_layer_norm.weight', 'encoder.encoder.layers.3.final_layer_norm.bias', 'encoder.encoder.layers.4.self_attn.k_proj.weight', 'encoder.encoder.layers.4.self_attn.k_proj.bias', 'encoder.encoder.layers.4.self_attn.v_proj.weight', 'encoder.encoder.layers.4.self_attn.v_proj.bias', 'encoder.encoder.layers.4.self_attn.q_proj.weight', 'encoder.encoder.layers.4.self_attn.q_proj.bias', 'encoder.encoder.layers.4.self_attn.out_proj.weight', 'encoder.encoder.layers.4.self_attn.out_proj.bias', 'encoder.encoder.layers.4.self_attn_layer_norm.weight', 'encoder.encoder.layers.4.self_attn_layer_norm.bias', 'encoder.encoder.layers.4.fc1.weight', 'encoder.encoder.layers.4.fc1.bias', 'encoder.encoder.layers.4.fc2.weight', 'encoder.encoder.layers.4.fc2.bias', 'encoder.encoder.layers.4.final_layer_norm.weight', 'encoder.encoder.layers.4.final_layer_norm.bias', 'encoder.encoder.layers.5.self_attn.k_proj.weight', 'encoder.encoder.layers.5.self_attn.k_proj.bias', 'encoder.encoder.layers.5.self_attn.v_proj.weight', 'encoder.encoder.layers.5.self_attn.v_proj.bias', 'encoder.encoder.layers.5.self_attn.q_proj.weight', 'encoder.encoder.layers.5.self_attn.q_proj.bias', 'encoder.encoder.layers.5.self_attn.out_proj.weight', 'encoder.encoder.layers.5.self_attn.out_proj.bias', 'encoder.encoder.layers.5.self_attn_layer_norm.weight', 'encoder.encoder.layers.5.self_attn_layer_norm.bias', 'encoder.encoder.layers.5.fc1.weight', 'encoder.encoder.layers.5.fc1.bias', 'encoder.encoder.layers.5.fc2.weight', 'encoder.encoder.layers.5.fc2.bias', 'encoder.encoder.layers.5.final_layer_norm.weight', 'encoder.encoder.layers.5.final_layer_norm.bias', 'encoder.encoder.layernorm_embedding.weight', 'encoder.encoder.layernorm_embedding.bias', 'encoder.decoder.embed_tokens.weight', 'encoder.decoder.embed_positions.weight', 'encoder.decoder.layers.0.self_attn.k_proj.weight', 'encoder.decoder.layers.0.self_attn.k_proj.bias', 'encoder.decoder.layers.0.self_attn.v_proj.weight', 'encoder.decoder.layers.0.self_attn.v_proj.bias', 'encoder.decoder.layers.0.self_attn.q_proj.weight', 'encoder.decoder.layers.0.self_attn.q_proj.bias', 'encoder.decoder.layers.0.self_attn.out_proj.weight', 'encoder.decoder.layers.0.self_attn.out_proj.bias', 'encoder.decoder.layers.0.self_attn_layer_norm.weight', 'encoder.decoder.layers.0.self_attn_layer_norm.bias', 'encoder.decoder.layers.0.encoder_attn.k_proj.weight', 'encoder.decoder.layers.0.encoder_attn.k_proj.bias', 'encoder.decoder.layers.0.encoder_attn.v_proj.weight', 'encoder.decoder.layers.0.encoder_attn.v_proj.bias', 'encoder.decoder.layers.0.encoder_attn.q_proj.weight', 'encoder.decoder.layers.0.encoder_attn.q_proj.bias', 'encoder.decoder.layers.0.encoder_attn.out_proj.weight', 'encoder.decoder.layers.0.encoder_attn.out_proj.bias', 'encoder.decoder.layers.0.encoder_attn_layer_norm.weight', 'encoder.decoder.layers.0.encoder_attn_layer_norm.bias', 'encoder.decoder.layers.0.fc1.weight', 'encoder.decoder.layers.0.fc1.bias', 'encoder.decoder.layers.0.fc2.weight', 'encoder.decoder.layers.0.fc2.bias', 'encoder.decoder.layers.0.final_layer_norm.weight', 'encoder.decoder.layers.0.final_layer_norm.bias', 'encoder.decoder.layers.1.self_attn.k_proj.weight', 'encoder.decoder.layers.1.self_attn.k_proj.bias', 'encoder.decoder.layers.1.self_attn.v_proj.weight', 'encoder.decoder.layers.1.self_attn.v_proj.bias', 'encoder.decoder.layers.1.self_attn.q_proj.weight', 'encoder.decoder.layers.1.self_attn.q_proj.bias', 'encoder.decoder.layers.1.self_attn.out_proj.weight', 'encoder.decoder.layers.1.self_attn.out_proj.bias', 'encoder.decoder.layers.1.self_attn_layer_norm.weight', 'encoder.decoder.layers.1.self_attn_layer_norm.bias', 'encoder.decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.decoder.layers.1.encoder_attn.k_proj.bias', 'encoder.decoder.layers.1.encoder_attn.v_proj.weight', 'encoder.decoder.layers.1.encoder_attn.v_proj.bias', 'encoder.decoder.layers.1.encoder_attn.q_proj.weight', 'encoder.decoder.layers.1.encoder_attn.q_proj.bias', 'encoder.decoder.layers.1.encoder_attn.out_proj.weight', 'encoder.decoder.layers.1.encoder_attn.out_proj.bias', 'encoder.decoder.layers.1.encoder_attn_layer_norm.weight', 'encoder.decoder.layers.1.encoder_attn_layer_norm.bias', 'encoder.decoder.layers.1.fc1.weight', 'encoder.decoder.layers.1.fc1.bias', 'encoder.decoder.layers.1.fc2.weight', 'encoder.decoder.layers.1.fc2.bias', 'encoder.decoder.layers.1.final_layer_norm.weight', 'encoder.decoder.layers.1.final_layer_norm.bias', 'encoder.decoder.layers.2.self_attn.k_proj.weight', 'encoder.decoder.layers.2.self_attn.k_proj.bias', 'encoder.decoder.layers.2.self_attn.v_proj.weight', 'encoder.decoder.layers.2.self_attn.v_proj.bias', 'encoder.decoder.layers.2.self_attn.q_proj.weight', 'encoder.decoder.layers.2.self_attn.q_proj.bias', 'encoder.decoder.layers.2.self_attn.out_proj.weight', 'encoder.decoder.layers.2.self_attn.out_proj.bias', 'encoder.decoder.layers.2.self_attn_layer_norm.weight', 'encoder.decoder.layers.2.self_attn_layer_norm.bias', 'encoder.decoder.layers.2.encoder_attn.k_proj.weight', 'encoder.decoder.layers.2.encoder_attn.k_proj.bias', 'encoder.decoder.layers.2.encoder_attn.v_proj.weight', 'encoder.decoder.layers.2.encoder_attn.v_proj.bias', 'encoder.decoder.layers.2.encoder_attn.q_proj.weight', 'encoder.decoder.layers.2.encoder_attn.q_proj.bias', 'encoder.decoder.layers.2.encoder_attn.out_proj.weight', 'encoder.decoder.layers.2.encoder_attn.out_proj.bias', 'encoder.decoder.layers.2.encoder_attn_layer_norm.weight', 'encoder.decoder.layers.2.encoder_attn_layer_norm.bias', 'encoder.decoder.layers.2.fc1.weight', 'encoder.decoder.layers.2.fc1.bias', 'encoder.decoder.layers.2.fc2.weight', 'encoder.decoder.layers.2.fc2.bias', 'encoder.decoder.layers.2.final_layer_norm.weight', 'encoder.decoder.layers.2.final_layer_norm.bias', 'encoder.decoder.layers.3.self_attn.k_proj.weight', 'encoder.decoder.layers.3.self_attn.k_proj.bias', 'encoder.decoder.layers.3.self_attn.v_proj.weight', 'encoder.decoder.layers.3.self_attn.v_proj.bias', 'encoder.decoder.layers.3.self_attn.q_proj.weight', 'encoder.decoder.layers.3.self_attn.q_proj.bias', 'encoder.decoder.layers.3.self_attn.out_proj.weight', 'encoder.decoder.layers.3.self_attn.out_proj.bias', 'encoder.decoder.layers.3.self_attn_layer_norm.weight', 'encoder.decoder.layers.3.self_attn_layer_norm.bias', 'encoder.decoder.layers.3.encoder_attn.k_proj.weight', 'encoder.decoder.layers.3.encoder_attn.k_proj.bias', 'encoder.decoder.layers.3.encoder_attn.v_proj.weight', 'encoder.decoder.layers.3.encoder_attn.v_proj.bias', 'encoder.decoder.layers.3.encoder_attn.q_proj.weight', 'encoder.decoder.layers.3.encoder_attn.q_proj.bias', 'encoder.decoder.layers.3.encoder_attn.out_proj.weight', 'encoder.decoder.layers.3.encoder_attn.out_proj.bias', 'encoder.decoder.layers.3.encoder_attn_layer_norm.weight', 'encoder.decoder.layers.3.encoder_attn_layer_norm.bias', 'encoder.decoder.layers.3.fc1.weight', 'encoder.decoder.layers.3.fc1.bias', 'encoder.decoder.layers.3.fc2.weight', 'encoder.decoder.layers.3.fc2.bias', 'encoder.decoder.layers.3.final_layer_norm.weight', 'encoder.decoder.layers.3.final_layer_norm.bias', 'encoder.decoder.layers.4.self_attn.k_proj.weight', 'encoder.decoder.layers.4.self_attn.k_proj.bias', 'encoder.decoder.layers.4.self_attn.v_proj.weight', 'encoder.decoder.layers.4.self_attn.v_proj.bias', 'encoder.decoder.layers.4.self_attn.q_proj.weight', 'encoder.decoder.layers.4.self_attn.q_proj.bias', 'encoder.decoder.layers.4.self_attn.out_proj.weight', 'encoder.decoder.layers.4.self_attn.out_proj.bias', 'encoder.decoder.layers.4.self_attn_layer_norm.weight', 'encoder.decoder.layers.4.self_attn_layer_norm.bias', 'encoder.decoder.layers.4.encoder_attn.k_proj.weight', 'encoder.decoder.layers.4.encoder_attn.k_proj.bias', 'encoder.decoder.layers.4.encoder_attn.v_proj.weight', 'encoder.decoder.layers.4.encoder_attn.v_proj.bias', 'encoder.decoder.layers.4.encoder_attn.q_proj.weight', 'encoder.decoder.layers.4.encoder_attn.q_proj.bias', 'encoder.decoder.layers.4.encoder_attn.out_proj.weight', 'encoder.decoder.layers.4.encoder_attn.out_proj.bias', 'encoder.decoder.layers.4.encoder_attn_layer_norm.weight', 'encoder.decoder.layers.4.encoder_attn_layer_norm.bias', 'encoder.decoder.layers.4.fc1.weight', 'encoder.decoder.layers.4.fc1.bias', 'encoder.decoder.layers.4.fc2.weight', 'encoder.decoder.layers.4.fc2.bias', 'encoder.decoder.layers.4.final_layer_norm.weight', 'encoder.decoder.layers.4.final_layer_norm.bias', 'encoder.decoder.layers.5.self_attn.k_proj.weight', 'encoder.decoder.layers.5.self_attn.k_proj.bias', 'encoder.decoder.layers.5.self_attn.v_proj.weight', 'encoder.decoder.layers.5.self_attn.v_proj.bias', 'encoder.decoder.layers.5.self_attn.q_proj.weight', 'encoder.decoder.layers.5.self_attn.q_proj.bias', 'encoder.decoder.layers.5.self_attn.out_proj.weight', 'encoder.decoder.layers.5.self_attn.out_proj.bias', 'encoder.decoder.layers.5.self_attn_layer_norm.weight', 'encoder.decoder.layers.5.self_attn_layer_norm.bias', 'encoder.decoder.layers.5.encoder_attn.k_proj.weight', 'encoder.decoder.layers.5.encoder_attn.k_proj.bias', 'encoder.decoder.layers.5.encoder_attn.v_proj.weight', 'encoder.decoder.layers.5.encoder_attn.v_proj.bias', 'encoder.decoder.layers.5.encoder_attn.q_proj.weight', 'encoder.decoder.layers.5.encoder_attn.q_proj.bias', 'encoder.decoder.layers.5.encoder_attn.out_proj.weight', 'encoder.decoder.layers.5.encoder_attn.out_proj.bias', 'encoder.decoder.layers.5.encoder_attn_layer_norm.weight', 'encoder.decoder.layers.5.encoder_attn_layer_norm.bias', 'encoder.decoder.layers.5.fc1.weight', 'encoder.decoder.layers.5.fc1.bias', 'encoder.decoder.layers.5.fc2.weight', 'encoder.decoder.layers.5.fc2.bias', 'encoder.decoder.layers.5.final_layer_norm.weight', 'encoder.decoder.layers.5.final_layer_norm.bias', 'encoder.decoder.layernorm_embedding.weight', 'encoder.decoder.layernorm_embedding.bias', 'decoder.final_logits_bias', 'decoder.model.shared.weight', 'decoder.model.encoder.embed_tokens.weight', 'decoder.model.encoder.embed_positions.weight', 'decoder.model.encoder.layers.0.self_attn.k_proj.weight', 'decoder.model.encoder.layers.0.self_attn.k_proj.bias', 'decoder.model.encoder.layers.0.self_attn.v_proj.weight', 'decoder.model.encoder.layers.0.self_attn.v_proj.bias', 'decoder.model.encoder.layers.0.self_attn.q_proj.weight', 'decoder.model.encoder.layers.0.self_attn.q_proj.bias', 'decoder.model.encoder.layers.0.self_attn.out_proj.weight', 'decoder.model.encoder.layers.0.self_attn.out_proj.bias', 'decoder.model.encoder.layers.0.self_attn_layer_norm.weight', 'decoder.model.encoder.layers.0.self_attn_layer_norm.bias', 'decoder.model.encoder.layers.0.fc1.weight', 'decoder.model.encoder.layers.0.fc1.bias', 'decoder.model.encoder.layers.0.fc2.weight', 'decoder.model.encoder.layers.0.fc2.bias', 'decoder.model.encoder.layers.0.final_layer_norm.weight', 'decoder.model.encoder.layers.0.final_layer_norm.bias', 'decoder.model.encoder.layers.1.self_attn.k_proj.weight', 'decoder.model.encoder.layers.1.self_attn.k_proj.bias', 'decoder.model.encoder.layers.1.self_attn.v_proj.weight', 'decoder.model.encoder.layers.1.self_attn.v_proj.bias', 'decoder.model.encoder.layers.1.self_attn.q_proj.weight', 'decoder.model.encoder.layers.1.self_attn.q_proj.bias', 'decoder.model.encoder.layers.1.self_attn.out_proj.weight', 'decoder.model.encoder.layers.1.self_attn.out_proj.bias', 'decoder.model.encoder.layers.1.self_attn_layer_norm.weight', 'decoder.model.encoder.layers.1.self_attn_layer_norm.bias', 'decoder.model.encoder.layers.1.fc1.weight', 'decoder.model.encoder.layers.1.fc1.bias', 'decoder.model.encoder.layers.1.fc2.weight', 'decoder.model.encoder.layers.1.fc2.bias', 'decoder.model.encoder.layers.1.final_layer_norm.weight', 'decoder.model.encoder.layers.1.final_layer_norm.bias', 'decoder.model.encoder.layers.2.self_attn.k_proj.weight', 'decoder.model.encoder.layers.2.self_attn.k_proj.bias', 'decoder.model.encoder.layers.2.self_attn.v_proj.weight', 'decoder.model.encoder.layers.2.self_attn.v_proj.bias', 'decoder.model.encoder.layers.2.self_attn.q_proj.weight', 'decoder.model.encoder.layers.2.self_attn.q_proj.bias', 'decoder.model.encoder.layers.2.self_attn.out_proj.weight', 'decoder.model.encoder.layers.2.self_attn.out_proj.bias', 'decoder.model.encoder.layers.2.self_attn_layer_norm.weight', 'decoder.model.encoder.layers.2.self_attn_layer_norm.bias', 'decoder.model.encoder.layers.2.fc1.weight', 'decoder.model.encoder.layers.2.fc1.bias', 'decoder.model.encoder.layers.2.fc2.weight', 'decoder.model.encoder.layers.2.fc2.bias', 'decoder.model.encoder.layers.2.final_layer_norm.weight', 'decoder.model.encoder.layers.2.final_layer_norm.bias', 'decoder.model.encoder.layers.3.self_attn.k_proj.weight', 'decoder.model.encoder.layers.3.self_attn.k_proj.bias', 'decoder.model.encoder.layers.3.self_attn.v_proj.weight', 'decoder.model.encoder.layers.3.self_attn.v_proj.bias', 'decoder.model.encoder.layers.3.self_attn.q_proj.weight', 'decoder.model.encoder.layers.3.self_attn.q_proj.bias', 'decoder.model.encoder.layers.3.self_attn.out_proj.weight', 'decoder.model.encoder.layers.3.self_attn.out_proj.bias', 'decoder.model.encoder.layers.3.self_attn_layer_norm.weight', 'decoder.model.encoder.layers.3.self_attn_layer_norm.bias', 'decoder.model.encoder.layers.3.fc1.weight', 'decoder.model.encoder.layers.3.fc1.bias', 'decoder.model.encoder.layers.3.fc2.weight', 'decoder.model.encoder.layers.3.fc2.bias', 'decoder.model.encoder.layers.3.final_layer_norm.weight', 'decoder.model.encoder.layers.3.final_layer_norm.bias', 'decoder.model.encoder.layers.4.self_attn.k_proj.weight', 'decoder.model.encoder.layers.4.self_attn.k_proj.bias', 'decoder.model.encoder.layers.4.self_attn.v_proj.weight', 'decoder.model.encoder.layers.4.self_attn.v_proj.bias', 'decoder.model.encoder.layers.4.self_attn.q_proj.weight', 'decoder.model.encoder.layers.4.self_attn.q_proj.bias', 'decoder.model.encoder.layers.4.self_attn.out_proj.weight', 'decoder.model.encoder.layers.4.self_attn.out_proj.bias', 'decoder.model.encoder.layers.4.self_attn_layer_norm.weight', 'decoder.model.encoder.layers.4.self_attn_layer_norm.bias', 'decoder.model.encoder.layers.4.fc1.weight', 'decoder.model.encoder.layers.4.fc1.bias', 'decoder.model.encoder.layers.4.fc2.weight', 'decoder.model.encoder.layers.4.fc2.bias', 'decoder.model.encoder.layers.4.final_layer_norm.weight', 'decoder.model.encoder.layers.4.final_layer_norm.bias', 'decoder.model.encoder.layers.5.self_attn.k_proj.weight', 'decoder.model.encoder.layers.5.self_attn.k_proj.bias', 'decoder.model.encoder.layers.5.self_attn.v_proj.weight', 'decoder.model.encoder.layers.5.self_attn.v_proj.bias', 'decoder.model.encoder.layers.5.self_attn.q_proj.weight', 'decoder.model.encoder.layers.5.self_attn.q_proj.bias', 'decoder.model.encoder.layers.5.self_attn.out_proj.weight', 'decoder.model.encoder.layers.5.self_attn.out_proj.bias', 'decoder.model.encoder.layers.5.self_attn_layer_norm.weight', 'decoder.model.encoder.layers.5.self_attn_layer_norm.bias', 'decoder.model.encoder.layers.5.fc1.weight', 'decoder.model.encoder.layers.5.fc1.bias', 'decoder.model.encoder.layers.5.fc2.weight', 'decoder.model.encoder.layers.5.fc2.bias', 'decoder.model.encoder.layers.5.final_layer_norm.weight', 'decoder.model.encoder.layers.5.final_layer_norm.bias', 'decoder.model.encoder.layernorm_embedding.weight', 'decoder.model.encoder.layernorm_embedding.bias', 'decoder.model.decoder.embed_tokens.weight', 'decoder.model.decoder.embed_positions.weight', 'decoder.model.decoder.layers.0.self_attn.k_proj.weight', 'decoder.model.decoder.layers.0.self_attn.k_proj.bias', 'decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'decoder.model.decoder.layers.0.self_attn.v_proj.bias', 'decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'decoder.model.decoder.layers.0.self_attn.q_proj.bias', 'decoder.model.decoder.layers.0.self_attn.out_proj.weight', 'decoder.model.decoder.layers.0.self_attn.out_proj.bias', 'decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.fc1.weight', 'decoder.model.decoder.layers.0.fc1.bias', 'decoder.model.decoder.layers.0.fc2.weight', 'decoder.model.decoder.layers.0.fc2.bias', 'decoder.model.decoder.layers.0.final_layer_norm.weight', 'decoder.model.decoder.layers.0.final_layer_norm.bias', 'decoder.model.decoder.layers.1.self_attn.k_proj.weight', 'decoder.model.decoder.layers.1.self_attn.k_proj.bias', 'decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'decoder.model.decoder.layers.1.self_attn.v_proj.bias', 'decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'decoder.model.decoder.layers.1.self_attn.q_proj.bias', 'decoder.model.decoder.layers.1.self_attn.out_proj.weight', 'decoder.model.decoder.layers.1.self_attn.out_proj.bias', 'decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.fc1.weight', 'decoder.model.decoder.layers.1.fc1.bias', 'decoder.model.decoder.layers.1.fc2.weight', 'decoder.model.decoder.layers.1.fc2.bias', 'decoder.model.decoder.layers.1.final_layer_norm.weight', 'decoder.model.decoder.layers.1.final_layer_norm.bias', 'decoder.model.decoder.layers.2.self_attn.k_proj.weight', 'decoder.model.decoder.layers.2.self_attn.k_proj.bias', 'decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'decoder.model.decoder.layers.2.self_attn.v_proj.bias', 'decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'decoder.model.decoder.layers.2.self_attn.q_proj.bias', 'decoder.model.decoder.layers.2.self_attn.out_proj.weight', 'decoder.model.decoder.layers.2.self_attn.out_proj.bias', 'decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.fc1.weight', 'decoder.model.decoder.layers.2.fc1.bias', 'decoder.model.decoder.layers.2.fc2.weight', 'decoder.model.decoder.layers.2.fc2.bias', 'decoder.model.decoder.layers.2.final_layer_norm.weight', 'decoder.model.decoder.layers.2.final_layer_norm.bias', 'decoder.model.decoder.layers.3.self_attn.k_proj.weight', 'decoder.model.decoder.layers.3.self_attn.k_proj.bias', 'decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'decoder.model.decoder.layers.3.self_attn.v_proj.bias', 'decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'decoder.model.decoder.layers.3.self_attn.q_proj.bias', 'decoder.model.decoder.layers.3.self_attn.out_proj.weight', 'decoder.model.decoder.layers.3.self_attn.out_proj.bias', 'decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.fc1.weight', 'decoder.model.decoder.layers.3.fc1.bias', 'decoder.model.decoder.layers.3.fc2.weight', 'decoder.model.decoder.layers.3.fc2.bias', 'decoder.model.decoder.layers.3.final_layer_norm.weight', 'decoder.model.decoder.layers.3.final_layer_norm.bias', 'decoder.model.decoder.layers.4.self_attn.k_proj.weight', 'decoder.model.decoder.layers.4.self_attn.k_proj.bias', 'decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'decoder.model.decoder.layers.4.self_attn.v_proj.bias', 'decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'decoder.model.decoder.layers.4.self_attn.q_proj.bias', 'decoder.model.decoder.layers.4.self_attn.out_proj.weight', 'decoder.model.decoder.layers.4.self_attn.out_proj.bias', 'decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.fc1.weight', 'decoder.model.decoder.layers.4.fc1.bias', 'decoder.model.decoder.layers.4.fc2.weight', 'decoder.model.decoder.layers.4.fc2.bias', 'decoder.model.decoder.layers.4.final_layer_norm.weight', 'decoder.model.decoder.layers.4.final_layer_norm.bias', 'decoder.model.decoder.layers.5.self_attn.k_proj.weight', 'decoder.model.decoder.layers.5.self_attn.k_proj.bias', 'decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'decoder.model.decoder.layers.5.self_attn.v_proj.bias', 'decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'decoder.model.decoder.layers.5.self_attn.q_proj.bias', 'decoder.model.decoder.layers.5.self_attn.out_proj.weight', 'decoder.model.decoder.layers.5.self_attn.out_proj.bias', 'decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.fc1.weight', 'decoder.model.decoder.layers.5.fc1.bias', 'decoder.model.decoder.layers.5.fc2.weight', 'decoder.model.decoder.layers.5.fc2.bias', 'decoder.model.decoder.layers.5.final_layer_norm.weight', 'decoder.model.decoder.layers.5.final_layer_norm.bias', 'decoder.model.decoder.layernorm_embedding.weight', 'decoder.model.decoder.layernorm_embedding.bias', 'decoder.lm_head.weight'])\n"]}]},{"cell_type":"markdown","source":["### Model Hyperparamters Fine Tuning :  \n"," \n","In intial attempts we observed that generated text length was very varying in comparison to golden text. \n","1.  Sometimes longer input sentences were losing the words during translation. \n","2.  Small input sentences has longer translated sentences\n","3.  Smaller sentences has meaningless words added in the end ie context of the input sentence was gettung lost.\n","\n","To address this, \n","1. we adjusted the decoding parameters, such as the length penalty or the minimum and maximum length constraints, to encourage the model to generate appropriate length of text. We added length penalization of 2 to discourage longer generation of sentence without meaningful context. \n","2. Instead of sampling only from the most likely K words, we ended up using Top-p sampling to choose from the smallest possible set of words whose cumulative probability exceeds the probability p. \n","3. We set this value high inorder to get high probablistic word. \n","4. We experimented with beam search size from 3-10 and observed that 5 gave us reasonble translated text\n","\n","After fine tuning generate method,we observed that model started performing little well in case of short sentences."],"metadata":{"id":"E3Phlcnw8HcD"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","def get_max_length(input_sequence):\n","    # Return a multiple of the input sequence length as the maximum length\n","    return int(len(input_sequence) * 1.75)\n","\n","def get_min_length(input_sequence):\n","    # Return a multiple of the input sequence length as the maximum length\n","    return int(len(input_sequence) *0.25)\n","\n","bart_model=bart_model.cuda()\n","def validate(model, val_loader, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE, optimizer_type='Adam', epochs=NUM_EPOCHS,verbose=True):\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate) if optimizer_type == 'Adam' else optim.SGD(model.parameters(), lr=learning_rate)\n","    model.eval()\n","    \n","    total_tokens = 0\n","    val_losses=[]\n","    generated_texts = []\n","    gold_texts = []\n","    t0 = time.time()\n","\n","    with torch.no_grad():\n","      for epoch in range(epochs):\n","        print ('######  Epoch {}/{} ######'.format(epoch+1, epochs))\n","        epoch_loss=0\n","        for step,batch in enumerate(val_loader):\n","            src = batch['toxic_ids'].cuda()\n","            tgt = batch['non_toxic_ids'].cuda()\n","            masked_src=batch['masked_toxic_ids'].cuda()\n","\n","            # Perform style transfer       \n","            # generate translations\n","            generated_ids = model.decoder.generate(\n","                input_ids=masked_src.cuda(),\n","                max_length=get_max_length(masked_src.cuda()),\n","                min_length=get_min_length(tgt.cuda()),\n","                num_beams=5,\n","                early_stopping=True,\n","                top_k=0, \n","                length_penalty=2, #1.2\n","                repetition_penalty=1,\n","                no_repeat_ngram_size=5,\n","                top_p = 0.95,\n","                temperature=0.8,\n","                decoder_start_token_id=bart_config.decoder_start_token_id\n","            )\n","\n","            generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","            generated_texts.extend(generated_text)\n","\n","            gold_text = tokenizer.batch_decode(tgt[:, 1:], skip_special_tokens=True)\n","            gold_texts.extend(gold_text)\n","       \n","            # Compute loss\n","            outputs=bart_model(input_ids=src, masked_input=masked_src)\n","\n","            loss = loss_fn(outputs, tgt.cuda())               \n","            epoch_loss += loss.item()\n","            \n","        # Report progress every batch\n","        if verbose and step % 1 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, step+1, len(val_loader), loss.item()))\n","\n","        avg_loss = epoch_loss / len(val_loader)\n","        val_losses.append(avg_loss)\n","\n","        elapsed_time = time.time() - t0\n","        if verbose:\n","            print('Epoch [{}/{}], Average Loss: {:.4f}, Elapsed Time: {}'.format(epoch+1, epochs, avg_loss, format_time(elapsed_time)))\n","\n","    return val_losses,avg_loss, generated_texts, gold_texts\n","\n","val_loss,avg_loss, generated_texts, gold_texts=validate(bart_model, val_loader, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE, optimizer_type='Adam') #'SGD'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ez1kUWEcXi-F","outputId":"9f64c0aa-4c11-4ced-826d-b82ef0199105","executionInfo":{"status":"ok","timestamp":1680464204807,"user_tz":420,"elapsed":57396,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["######  Epoch 1/4 ######\n","Epoch [1/4], Step [13/13], Loss: 4.7482\n","Epoch [1/4], Average Loss: 5.1847, Elapsed Time: 0:00:17\n","######  Epoch 2/4 ######\n","Epoch [2/4], Step [13/13], Loss: 4.7482\n","Epoch [2/4], Average Loss: 5.1847, Elapsed Time: 0:00:30\n","######  Epoch 3/4 ######\n","Epoch [3/4], Step [13/13], Loss: 4.7482\n","Epoch [3/4], Average Loss: 5.1847, Elapsed Time: 0:00:43\n","######  Epoch 4/4 ######\n","Epoch [4/4], Step [13/13], Loss: 4.7482\n","Epoch [4/4], Average Loss: 5.1847, Elapsed Time: 0:00:57\n"]}]},{"cell_type":"code","source":["val_loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYC0GFzVgC27","outputId":"f76ba56f-733e-44d9-f713-422af4469074","executionInfo":{"status":"ok","timestamp":1680464237478,"user_tz":420,"elapsed":25,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[5.184738966134878, 5.184738966134878, 5.184738966134878, 5.184738966134878]"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["dev=pd.read_csv(dev_path,sep=\",\", header=None) \n","\n","for i in range(len(generated_texts[:10])):\n","  print(i,'*'*20)\n","  print(' Original text   :: ',dev[1][i])\n","  print(' Gold text       :: ',gold_texts[i])\n","  print(' Translated text :: ',generated_texts[i])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bAn7pvYaiKK","outputId":"589933bb-d97d-422e-a4f6-92898a1e399e","executionInfo":{"status":"ok","timestamp":1680464241067,"user_tz":420,"elapsed":8,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["0 ********************\n"," Original text   ::  anyone who thinks canceling a pipeline and handcuffing fossil energy production in the us would have no effect on gas prices is a fucking moron\n"," Gold text       ::  anyone who thinks canceling a pipeline and handcuffing fossil energy production in the us would have no effect on gas prices is not thinking about this\n"," Translated text ::  anyone who thinks canceling a pipeline and handcuffing fossil energy production in the us would have no effect on gas prices\n","1 ********************\n"," Original text   ::  my favorite part is that youre still madly replying as if its going to quell your cousin fucking urges\n"," Gold text       ::  my favorite part is that you are still replying\n"," Translated text ::  my favorite part is that youre still madly replying as if its going to quell your cousin\n","2 ********************\n"," Original text   ::  but did you try it lol that and medical marijuana helped my cousins tinnitus she used to puke cause she would be so dizzy\n"," Gold text       ::  have you tried it that helped my cousin s tinnitus she used to puke cause she would be so dizzy\n"," Translated text ::  but did you try it before that and medical marijuana helped my cousins tinnitus she used to puke cause she would be\n","3 ********************\n"," Original text   ::  by the downvotes i see there are a bunch of uneducated trumpers here who dont understand oil you probably think bideg contols gas prices too \n"," Gold text       ::  by the downvotes i see there are a bunch of trump supporters here who dont understand oil you probably think bideg contols gas prices too\n"," Translated text ::  by the downvotes i see there are a bunch of uneducated trumpers here who dont understand oil you probably think bide\n","4 ********************\n"," Original text   ::  cuckold carlson is a problem\n"," Gold text       ::  carlson is the problem\n"," Translated text ::  why carlson is a problem\n","5 ********************\n"," Original text   ::  shut tf up lmfaoooo youre soooo victimized\n"," Gold text       ::  please don t say anything you sound soooo victimized\n"," Translated text ::  shut tf up lmfaoooo youre soooo victimized\n","6 ********************\n"," Original text   ::  remember when tucker cucked him on national tv and he begged for forgiveness dude is the definition of weak\n"," Gold text       ::  remember when tucker messed with him on national tv and he had to ask for forgiveness that was weak\n"," Translated text ::  remember when tucker punched him on national tv and he begged for forgiveness dude is\n","7 ********************\n"," Original text   ::  it s not the hat that s offensive it s the smell of your breath from licking so much shit off of boots\n"," Gold text       ::  it s not the hat that s offensive it s the smell of your breath\n"," Translated text ::  it s not the hat that s offensive it s the smell of your breath from licking so much of it off of\n","8 ********************\n"," Original text   ::  i was gonna retort but looks like god already did you in with the als have a shitty life\n"," Gold text       ::  i was gonna retort but i feel bad because you have als\n"," Translated text ::  i was gonna retort but looks like i already did you in with the als\n","9 ********************\n"," Original text   ::  hilariously nicotine iirc actually suppresses the cilia it s like a giant plot by tobacco to screw your lungs\n"," Gold text       ::  hilariously nicotine iirc actually suppresses the cilia it s like a giant plot by tobacco to mess up your lungs\n"," Translated text ::  hilariously nicotine iirc actually suppresses the cilia it s like a giant plot by tobacco to stop the growth of\n"]}]},{"cell_type":"code","source":["import seaborn as sns\n","### Plot Training & Validation Loss\n","data = {'Epoch': range(1, 5), 'Training Loss': train_losses, 'Validation Loss': val_loss}\n","df = pd.DataFrame(data)\n","\n","fig, ax = plt.subplots(figsize=(10, 6))\n","sns.set_style('whitegrid')\n","sns.set(font_scale=1.5)\n","\n","# Plot train & val loss\n","sns.lineplot(data=df, x=\"Epoch\", y=\"Training Loss\", label=\"Training Loss\", marker='o')\n","sns.lineplot(data=df, x=\"Epoch\", y=\"Validation Loss\", label=\"Validation Loss\", marker='o')\n","\n","# set the axis labels and title\n","ax.set(xlabel='Epoch', ylabel='Loss')\n","ax.set_title('Training and Validation Loss', fontsize=18)\n","\n","# set the legend and adjust its position\n","plt.legend(fontsize=14)\n","\n","# set the ticks fontsize\n","ax.tick_params(axis='both', which='major', labelsize=14)\n","# ax.grid(True, which='both', linestyle='--', color='lightgray')"],"metadata":{"id":"DwIX28xsI4CQ","colab":{"base_uri":"https://localhost:8080/","height":588},"outputId":"d79e4f72-c50e-49ba-b785-ca5f2b1db904","executionInfo":{"status":"ok","timestamp":1680464250633,"user_tz":420,"elapsed":733,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["test_data = MyDataset(test_path, MAX_SEQ_LEN,'test') \n","test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False,collate_fn=collate_fn)\n"],"metadata":{"id":"ngvdJlOC82ay","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8d63a847-1df1-4514-c25a-13a57661f91d","executionInfo":{"status":"ok","timestamp":1680464263227,"user_tz":420,"elapsed":8054,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," test  dataset preview .....\n","*******************************************\n","Input :: toxic        :: so maybe you should be more retarded\n","Input :: masked toxic :: so maybe you should be more <mask>\n","Input :: non toxic    :: so maybe you should be more backward\n","encoded length toxic : {'input_ids': [0, 2527, 2085, 47, 197, 28, 55, 47304, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","encoded length masked toxic:  {'input_ids': [0, 2527, 2085, 47, 197, 28, 55, 50264, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","encoded length non toxic:  {'input_ids': [0, 2527, 2085, 47, 197, 28, 55, 18173, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","*******************************************\n","total number of test  data processed :  199\n"]}]},{"cell_type":"code","source":["from nltk.translate.bleu_score import SmoothingFunction\n","import evaluate\n","\n","rouge = evaluate.load('rouge')\n","bleu = evaluate.load('bleu')\n","\n","\n","# Define a smoothing function for Rouge-L\n","smoothie = SmoothingFunction().method4\n","\n","def model_evaluate(model, data_loader):\n","    model.eval()\n","    total_loss = 0\n","    total_accuracy = 0\n","    total_precision = 0\n","    total_recall = 0\n","    total_f1 = 0\n","    # Calculate the evaluation metrics\n","    total_correct = 0\n","    total_predicted = 0\n","    total_gold = 0\n","    total_batches = 0\n","\n","    total_rouge1 = 0\n","    total_rouge2 = 0\n","    total_rougel = 0\n","    total_rougelsum = 0\n","    total_gen_len = 0\n","\n","    golden_texts=[]\n","    predicted_generated_text=[]\n","    bleu_scores = []\n","    rouge = Rouge()\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader):\n","            toxic_src = batch['toxic_ids'].cuda()\n","            non_toxic_tgt = batch['non_toxic_ids'].cuda()\n","            masked_toxic_src= batch['masked_toxic_ids'].cuda()\n","            \n","            generated_ids = model.decoder.generate(\n","                input_ids=masked_toxic_src.cuda(),\n","                max_length=get_max_length(masked_toxic_src.cuda()),\n","                min_length=get_min_length(non_toxic_tgt.cuda()),\n","                num_beams=5,\n","                early_stopping=True,\n","                top_k=0, \n","                length_penalty=2, #1.2\n","                repetition_penalty=1,\n","                no_repeat_ngram_size=5,\n","                top_p = 0.95,\n","                temperature=0.8,\n","                decoder_start_token_id=bart_config.decoder_start_token_id\n","            )\n","\n","            predicted_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","            predicted_generated_text.extend(predicted_text)\n","\n","            gold_text = tokenizer.batch_decode(non_toxic_tgt[:, 1:], skip_special_tokens=True)\n","            golden_texts.extend(gold_text)\n","\n","            #calculating cross entropy loss\n","            outputs=bart_model(input_ids=toxic_src, masked_input=masked_toxic_src)\n","            loss = loss_fn(outputs, non_toxic_tgt.cuda())\n","            \n","            total_loss += loss.item()\n","            \n","            for i, (predicted_sent, gold_sent) in enumerate(zip(predicted_generated_text, golden_texts)):\n","                # Calculate the number of correct tokens\n","                \n","                correct_tokens = sum([1 for p, g in zip(predicted_sent, gold_sent) if p == g])\n","                total_correct += correct_tokens\n","                \n","                # Calculate the number of predicted and gold tokens\n","                predicted_tokens = len(predicted_sent)\n","                total_predicted += predicted_tokens\n","                \n","                gold_tokens = len(gold_sent.split())\n","                total_gold += gold_tokens\n","               \n","            # Calculate average generated length\n","            total_gen_len += sum([len(sent.split()) for sent in predicted_text])\n","            total_batches += 1*len(predicted_text)\n","\n","\n","    print('number of records in test',total_batches)     \n","\n","    # Calculate average metrics for the entire dataset\n","    avg_loss = total_loss / total_batches\n","    avg_accuracy = total_correct / total_predicted if total_predicted > 0 else 0\n","    avg_precision = total_correct / total_predicted if total_predicted > 0 else 0\n","    avg_recall = total_correct / total_gold if total_gold > 0 else 0\n","    avg_f1 = 2 * avg_precision * avg_recall / (avg_precision + avg_recall) if avg_precision + avg_recall > 0 else 0\n","\n","    print(\"Loss: {:.4f}\".format(avg_loss))\n","    print(\"Accuracy: {:.4f}\".format(avg_accuracy))\n","    print(\"Precision: {:.4f}\".format(avg_precision))\n","    print(\"Recall: {:.4f}\".format(avg_recall))\n","    print(\"F1 Score: {:.4f}\".format(avg_f1))\n","\n","    return predicted_generated_text,golden_texts\n","\n","predicted_generated_text,golden_texts=model_evaluate(bart_model, test_loader)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L1OhZ3518pDx","outputId":"7c1cdb56-94ef-4aad-9833-0d897c439dfe","executionInfo":{"status":"ok","timestamp":1680464284590,"user_tz":420,"elapsed":14679,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":53,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:13<00:00,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["number of records in test 208\n","Loss: 0.3306\n","Accuracy: 0.3756\n","Precision: 0.3756\n","Recall: 2.0780\n","F1 Score: 0.6362\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["rouge.compute(predictions=predicted_generated_text,references=golden_texts)"],"metadata":{"id":"X15MsWAyE66A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"52c7e528-acd5-4bf9-fb80-901f303f1cba","executionInfo":{"status":"ok","timestamp":1680464332464,"user_tz":420,"elapsed":1136,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'rouge1': 0.6340497945456579,\n"," 'rouge2': 0.5140818255410882,\n"," 'rougeL': 0.6283098476980357,\n"," 'rougeLsum': 0.6288498180144824}"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["ROUGE-1 score: 0.634, which means that 63.4% of the unigrams in the generated text are also present in the reference text.\n","ROUGE-2 score: 0.514, which means that 51.4% of the bigrams in the generated text are also present in the reference text.\n","ROUGE-L score: 0.628, which means that 62.8% of the longest common subsequence between the generated text and the reference text is present in the reference text.  \n","ROUGE-Lsum score: 0.628, which is similar to ROUGE-L, but it also penalizes repeated words in the summary."],"metadata":{"id":"zt51_MvLWl3s"}},{"cell_type":"code","source":["print(bleu.compute(predictions=predicted_generated_text,references=golden_texts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6g0oQhefLxoQ","outputId":"cd6df8f8-7b4e-4139-9cb7-513b2b0552d9","executionInfo":{"status":"ok","timestamp":1680464412403,"user_tz":420,"elapsed":6,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["{'bleu': 0.536052683692575, 'precisions': [0.6722354813046937, 0.5689505637467476, 0.49571020019065776, 0.4355179704016913], 'brevity_penalty': 1.0, 'length_ratio': 1.0803609798023206, 'translation_length': 2514, 'reference_length': 2327}\n"]}]},{"cell_type":"markdown","source":["BLEU score: 0.536, which means that the machine-generated translation has 53.6% similarity with the reference translations.\n","Precisions: [0.6722354813046937, 0.5689505637467476, 0.49571020019065776, 0.4355179704016913], which represent the unigram, bigram, trigram, and 4-gram precision scores, respectively.\n","Brevity penalty: 1.0, which indicates that the length of the machine-generated translation is not significantly different from the length of the reference translations.\n","Length ratio: 1.08, which represents the ratio of the length of the machine-generated translation to the length of the reference translations.\n","Translation length: 2514, which is the length of the machine-generated translation.\n","Reference length: 2327, which is the average length of the reference translations.\n","Overall, the BLEU score of 0.536 suggests that the machine-generated translation has moderate similarity with the reference translations"],"metadata":{"id":"07L3NR3sW3JN"}},{"cell_type":"code","source":["df_test=pd.read_csv(test_path,sep=\",\", header=None) \n","\n","for i in range(len(predicted_generated_text[:5])):\n","  print(' Original text   :: ',df_test[1][i])\n","  print(' Gold text       :: ',df_test[2][i])\n","  print(' Translated text :: ',predicted_generated_text[i])\n","\n","# Create a new DataFrame with the generated texts\n","df_gen = pd.DataFrame({'bart_tuning_output': predicted_generated_text}) \n","\n","# Add the generated_text column to the df_test dataframe\n","df_bart_output = pd.concat([df_test, df_gen], axis=1)\n","\n","# Save the updated dataframe as a CSV file\n","df_bart_output=df_bart_output.drop([0],axis=1)\n","df_bart_output=df_bart_output[:-1]\n","df_bart_output.to_csv('/content/drive/MyDrive/Colab Notebooks/outputs/bart_tuning_output.csv',sep='\\t',index=False,header=['toxic_text','gold_text','bart_tuning_output'])"],"metadata":{"id":"kM6-ak9I7vBM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c47e2bef-1509-49d1-c5da-9a8f680e66dc","executionInfo":{"status":"ok","timestamp":1680464513731,"user_tz":420,"elapsed":6,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":[" Original text   ::  so maybe you should be more retarded\n"," Gold text       ::  so maybe you should be more backward\n"," Translated text ::  so maybe you should be more careful\n"," Original text   ::  theres a megathread for vaccine or covid related topics don t talk about that shit here idiot\n"," Gold text       ::  theres actually a megathread for vaccine or covid related topics don t talk about that stuff here\n"," Translated text ::  theres a megathread for vaccine or covid related topics don t talk about that here\n"," Original text   ::  the fuck you are on drugs\n"," Gold text       ::  uh you are not being realistic\n"," Translated text ::  the reason why you are on drugs\n"," Original text   ::  nta dump his ass tablemanners are no rocket science treating other people like human beings is no rocket science his still a child\n"," Gold text       ::  you should leave him tablemanners are no rocket science treating other people like human beings is no rocket science his still a child\n"," Translated text ::  nta dump his money on the tablemanners are no rocket science treating other people like human beings is no rocket science\n"," Original text   ::  youre soft as baby shit\n"," Gold text       ::  youre really soft\n"," Translated text ::  youre soft as baby\n"]}]},{"cell_type":"code","source":["output_file = '/content/drive/MyDrive/Colab Notebooks/outputs/bart_tuning_output.csv'\n","sample=pd.read_csv(output_file, sep=\"\\t\")\n","sample[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"zDx3X9eBsnXb","outputId":"9b30bf45-cb7a-4634-962e-e08f761c1677","executionInfo":{"status":"ok","timestamp":1680464516979,"user_tz":420,"elapsed":23,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          toxic_text  \\\n","0               so maybe you should be more retarded   \n","1  theres a megathread for vaccine or covid relat...   \n","2                          the fuck you are on drugs   \n","3  nta dump his ass tablemanners are no rocket sc...   \n","4                            youre soft as baby shit   \n","\n","                                           gold_text  \\\n","0               so maybe you should be more backward   \n","1  theres actually a megathread for vaccine or co...   \n","2                     uh you are not being realistic   \n","3  you should leave him tablemanners are no rocke...   \n","4                                  youre really soft   \n","\n","                                  bart_tuning_output  \n","0                so maybe you should be more careful  \n","1  theres a megathread for vaccine or covid relat...  \n","2                    the reason why you are on drugs  \n","3  nta dump his money on the tablemanners are no ...  \n","4                                 youre soft as baby  "],"text/html":["\n","  <div id=\"df-ac52bf7b-4c61-46c1-ac2c-74e10131a117\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>toxic_text</th>\n","      <th>gold_text</th>\n","      <th>bart_tuning_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>so maybe you should be more retarded</td>\n","      <td>so maybe you should be more backward</td>\n","      <td>so maybe you should be more careful</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>theres a megathread for vaccine or covid relat...</td>\n","      <td>theres actually a megathread for vaccine or co...</td>\n","      <td>theres a megathread for vaccine or covid relat...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>the fuck you are on drugs</td>\n","      <td>uh you are not being realistic</td>\n","      <td>the reason why you are on drugs</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>nta dump his ass tablemanners are no rocket sc...</td>\n","      <td>you should leave him tablemanners are no rocke...</td>\n","      <td>nta dump his money on the tablemanners are no ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>youre soft as baby shit</td>\n","      <td>youre really soft</td>\n","      <td>youre soft as baby</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac52bf7b-4c61-46c1-ac2c-74e10131a117')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ac52bf7b-4c61-46c1-ac2c-74e10131a117 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ac52bf7b-4c61-46c1-ac2c-74e10131a117');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/notebooks')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/models')"],"metadata":{"id":"Rz8VIRl7e0eL","executionInfo":{"status":"ok","timestamp":1680464519425,"user_tz":420,"elapsed":3,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["from DistilBertClassification import BertClassificationML, NonToxicScoreDataLoader, NonToxicScore\n","\n","# Load DistilBERT Classification Model to calculate NonToxicScore\n","score_model = BertClassificationML()\n","score_model = score_model.to(device)\n","\n","# Load training weights\n","pretrained_weights = torch.load('/content/drive/MyDrive/Colab Notebooks/models/DistilBertToxicClassification7.pth')\n","score_model.load_state_dict(pretrained_weights )"],"metadata":{"id":"nW_yJ19qgZmI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"38845f7a-b7e6-4a1d-81ef-d365b6add8f3","executionInfo":{"status":"ok","timestamp":1680464522925,"user_tz":420,"elapsed":1201,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":59,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["output_file = '/content/drive/MyDrive/Colab Notebooks/outputs/bart_tuning_output.csv'\n","output_col = 'bart_tuning_output'\n","\n","# Create Data Loader\n","score_loader = NonToxicScoreDataLoader(output_file, output_col)\n","\n","# Calculate NonToxicScore\n","bart_NonToxicScores, avg_score = NonToxicScore(score_loader, score_model)"],"metadata":{"id":"WVzkmqLJNG3h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e62181a5-96e3-4211-edce-3377a71c63fa","executionInfo":{"status":"ok","timestamp":1680464524872,"user_tz":420,"elapsed":1102,"user":{"displayName":"Shivangi Pandey","userId":"03692652119201342747"}}},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["{'NonToxicScore': 0.55948131894839}\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"247479d6915b4eb5946d76318777ef06":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffd2eeb7c0b44f9c9aae80a32998aa5f","IPY_MODEL_5246864ab21f46cfa41ac9967dde4781","IPY_MODEL_dd3f8462e0be453aa909a0da2a6f4ab4"],"layout":"IPY_MODEL_6b194fa74ec246c8af12d7d2037ce685"}},"ffd2eeb7c0b44f9c9aae80a32998aa5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2250b1f8de13464badc9818447ab44cd","placeholder":"â€‹","style":"IPY_MODEL_43b4da794d5c4b78ab15a4e1d9df6858","value":"Downloading (â€¦)olve/main/vocab.json: 100%"}},"5246864ab21f46cfa41ac9967dde4781":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05b93dc3d8d146eda44c3c583c4b8688","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ca611a654244484aea941e3bb7dd8f4","value":898823}},"dd3f8462e0be453aa909a0da2a6f4ab4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f4ccdfdd3d3440fa7280efe61caf1f1","placeholder":"â€‹","style":"IPY_MODEL_c2c62bdf0e634af7bf4eaa0685cee31d","value":" 899k/899k [00:00&lt;00:00, 20.1MB/s]"}},"6b194fa74ec246c8af12d7d2037ce685":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2250b1f8de13464badc9818447ab44cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43b4da794d5c4b78ab15a4e1d9df6858":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05b93dc3d8d146eda44c3c583c4b8688":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ca611a654244484aea941e3bb7dd8f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f4ccdfdd3d3440fa7280efe61caf1f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2c62bdf0e634af7bf4eaa0685cee31d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1bb2c2fa2694147a86259cbf3b8e6f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1e731dbab8ec47de9cc6dae743299a05","IPY_MODEL_f0f8750592604a10af0e0037a9306f6d","IPY_MODEL_91fcf5c26b434c468def649b20ec4587"],"layout":"IPY_MODEL_99e24f93bbdd43d682bb5538988a9458"}},"1e731dbab8ec47de9cc6dae743299a05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84489ba1107947f4aad96be3bf7e34a6","placeholder":"â€‹","style":"IPY_MODEL_de8a620064694039844226e4693a974f","value":"Downloading (â€¦)olve/main/merges.txt: 100%"}},"f0f8750592604a10af0e0037a9306f6d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b45da579ab114b2f88a90812f306438d","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e9bbfbdc39d423d8b67eb64c308a7d3","value":456318}},"91fcf5c26b434c468def649b20ec4587":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49bd0896a5b443b1b6a1a4402bab4a29","placeholder":"â€‹","style":"IPY_MODEL_2d47cdcfa2c94fec940a4490cf2b901d","value":" 456k/456k [00:00&lt;00:00, 14.2MB/s]"}},"99e24f93bbdd43d682bb5538988a9458":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84489ba1107947f4aad96be3bf7e34a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de8a620064694039844226e4693a974f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b45da579ab114b2f88a90812f306438d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e9bbfbdc39d423d8b67eb64c308a7d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49bd0896a5b443b1b6a1a4402bab4a29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d47cdcfa2c94fec940a4490cf2b901d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bbc0db1ff204b3ca0b7fd8ec9f03eb0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e91a4adf2da4434ac9c33eb0f18b1ac","IPY_MODEL_f2704b01fe424387ad8610ad32e2ac45","IPY_MODEL_1c251e5da0654cbb88c23a2b50fc4bd9"],"layout":"IPY_MODEL_fbc620e9ff9042dfbc7f8b72c3a4ee52"}},"8e91a4adf2da4434ac9c33eb0f18b1ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2e1ee941c8549afa4b69312026840a8","placeholder":"â€‹","style":"IPY_MODEL_659b824bc08842779f2eb6593bdd4b1f","value":"Downloading (â€¦)lve/main/config.json: 100%"}},"f2704b01fe424387ad8610ad32e2ac45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6155c0592924322a10a06506e202cd5","max":1716,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff361219466f4f0a803f3ab452d2226e","value":1716}},"1c251e5da0654cbb88c23a2b50fc4bd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e6c6524c5704ba8bdee65bcd4c28a7c","placeholder":"â€‹","style":"IPY_MODEL_88cafc20b5b94535a1d333a1ad5ddc99","value":" 1.72k/1.72k [00:00&lt;00:00, 44.6kB/s]"}},"fbc620e9ff9042dfbc7f8b72c3a4ee52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2e1ee941c8549afa4b69312026840a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"659b824bc08842779f2eb6593bdd4b1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6155c0592924322a10a06506e202cd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff361219466f4f0a803f3ab452d2226e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e6c6524c5704ba8bdee65bcd4c28a7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88cafc20b5b94535a1d333a1ad5ddc99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}