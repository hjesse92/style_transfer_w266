{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjesse92/style_transfer_w266/blob/main/Text_Style_Disentanglement_BERT_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "DeU99n6jOiBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmEhHFRUOdx9"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentencepiece rouge_score evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bkEq5OeOlCC",
        "outputId": "9f1f1c5c-1d4d-4afe-8eae-d10819c53509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 22 08:41:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('Number of GPU(s) available:', torch.cuda.device_count())\n",
        "    print('GPU device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqo-lSq2OmJ6",
        "outputId": "1c5ec0c3-f442-4c8d-fe2d-ae7df3923b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPU(s) available: 1\n",
            "GPU device name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import warning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DistilBertTokenizer, DistilBertModel, AdamW\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0_KiWVjOnc9",
        "outputId": "4753500d-4138-441a-dd1e-acf9de1bd733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1cf51a0250>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_file = 'drive/MyDrive/data/original-train.tsv'\n",
        "dev_file = 'drive/MyDrive/data/original-dev.tsv'\n",
        "test_file = 'drive/MyDrive/data/original-test.tsv'\n",
        "# df_train = pd.read_csv(train_file, sep='\\t')\n",
        "# df_dev = pd.read_csv(dev_file, sep='\\t')\n",
        "# df_test = pd.read_csv(test_file, sep='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXzIUfCROvcf",
        "outputId": "bd55d225-fc71-4dc2-acdc-3d967d458bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yVwhYeS7_CA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextStyleTransferDataset(Dataset):\n",
        "    def __init__(self, data_path, encoder_tokenizer, decoder_tokenizer, max_len):\n",
        "        self.data = []\n",
        "        self.encoder_tokenizer = encoder_tokenizer\n",
        "        self.decoder_tokenizer = decoder_tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.decoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n",
        "        self.decoder_tokenizer.padding_side = 'left'\n",
        "\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                source, target = line.strip().split('\\t')\n",
        "                self.data.append((source, target))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source, target = self.data[idx]\n",
        "\n",
        "        source_encoding = self.encoder_tokenizer(source, max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        target_encoding = self.encoder_tokenizer(target, max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        gpt_encoding = self.decoder_tokenizer(source, max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        target_gpt_encoding = self.decoder_tokenizer(target, max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "        return {'source_inputs': source_encoding,\n",
        "                'gpt_inputs': gpt_encoding,\n",
        "                'target_inputs': target_encoding,\n",
        "                'target_gpt_inputs': target_gpt_encoding}\n",
        "\n",
        "class TextStyleTransferModel(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(TextStyleTransferModel, self).__init__()\n",
        "        \n",
        "        self.bert_model_toxic = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.bert_model_nontoxic = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        \n",
        "        # We will use autoencoder architecture to deconstruct toxic/nontoxic latent spaces and reconstruct\n",
        "        self.toxic_encoder = nn.Sequential(\n",
        "            nn.Linear(768, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 4)\n",
        "        )\n",
        "        self.toxic_decoder = nn.Sequential(\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 768)\n",
        "        )\n",
        "\n",
        "        self.nontoxic_encoder = nn.Sequential(\n",
        "            nn.Linear(768, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 4)\n",
        "        )\n",
        "        self.nontoxic_decoder = nn.Sequential(\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 768)\n",
        "        )\n",
        "        self.style_transfer_decoder = nn.Sequential(\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 768)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # self.toxic_semantic = nn.Linear(in_features = self.bert_model_toxic.config.hidden_size,\n",
        "        #                                 out_features = semantic_latent_dim,\n",
        "        #                                 bias=True)\n",
        "        # self.toxic_style = nn.Linear(in_features = self.bert_model_toxic.config.hidden_size,\n",
        "        #                              out_features = style_latent_dim,\n",
        "        #                              bias=True)\n",
        "        \n",
        "        # self.nontoxic_semantic = nn.Linear(in_features = self.bert_model_nontoxic.config.hidden_size,\n",
        "        #                                    out_features = semantic_latent_dim,\n",
        "        #                                    bias=True)\n",
        "        # self.nontoxic_style = nn.Linear(in_features = self.bert_model_nontoxic.config.hidden_size,\n",
        "        #                                 out_features = style_latent_dim,\n",
        "        #                                 bias=True)\n",
        "\n",
        "        self.fusion = nn.Linear(in_features = 4,\n",
        "                                out_features = 4,\n",
        "                                bias=True)\n",
        "\n",
        "        # self.hidden1 = nn.Linear(768, 1536, bias=True)\n",
        "        # # self.hidden2 = nn.Linear(self.hidden1.in_features, int(5*self.hidden1.out_features))\n",
        "\n",
        "        # self.final_layer = nn.Linear(in_features = self.style_transfer_decoder.out_features,\n",
        "        #                              out_features = self.bert_model_toxic.config.vocab_size,\n",
        "        #                              bias=True)\n",
        "        \n",
        "        # self.mapping_hidden = nn.Linear(self.bert_model_toxic.config.vocab_size, self.gpt_model.config.vocab_size)\n",
        "        self.mapping_model = nn.Linear(768, self.gpt_model.config.vocab_size)\n",
        "\n",
        "        # This will be learned and saved in the model\n",
        "        self.nontoxic_latent_style = None\n",
        "\n",
        "\n",
        "    def forward(self, toxic_input_ids, toxic_attention_mask, nontoxic_input_ids=None, nontoxic_attention_mask=None):\n",
        "        # Encoding\n",
        "        toxic_bert = self.bert_model_toxic(input_ids=toxic_input_ids, attention_mask=toxic_attention_mask).last_hidden_state\n",
        "        nontoxic_bert = self.bert_model_nontoxic(input_ids=nontoxic_input_ids, attention_mask=nontoxic_attention_mask).last_hidden_state\n",
        "        \n",
        "        # Grab the CLS token or the mean of last layer\n",
        "        bert_pool_toxic = toxic_bert.mean(dim=1)\n",
        "        bert_pool_nontoxic = nontoxic_bert.mean(dim=1)\n",
        "        # content_latent = content_outputs[0]\n",
        "\n",
        "        toxic_latent_rep = self.toxic_encoder(bert_pool_toxic)\n",
        "        toxic_latent_semantic, toxic_latent_style = toxic_latent_rep[:, :2], toxic_latent_rep[:, 2:]\n",
        "\n",
        "        nontoxic_latent_rep = self.nontoxic_encoder(bert_pool_nontoxic)\n",
        "        nontoxic_latent_semantic, self.nontoxic_latent_style = nontoxic_latent_rep[:, :2], nontoxic_latent_rep[:, 2:]\n",
        "\n",
        "        toxic_reconstructed = self.toxic_decoder(toxic_latent_rep)\n",
        "        nontoxic_reconstructed = self.nontoxic_decoder(nontoxic_latent_rep)\n",
        "\n",
        "        fusion_latent = torch.cat((toxic_latent_semantic, self.nontoxic_latent_style),dim=1)\n",
        "        fusion_latent = self.fusion(fusion_latent)\n",
        "\n",
        "        style_transfer_reconstructed = self.style_transfer_decoder(fusion_latent)\n",
        "\n",
        "        mapping = self.mapping_model(style_transfer_reconstructed)\n",
        "        \n",
        "        mapped_embeddings = self.gpt_model.transformer.wte(mapping.argmax(-1))\n",
        "        mapping_repeated = mapped_embeddings.unsqueeze(1).repeat(1, 128, 1)\n",
        "        \n",
        "        gpt_outputs = self.gpt_model(inputs_embeds=mapping_repeated, return_dict=True)\n",
        "        gpt_logits = gpt_outputs.logits\n",
        "\n",
        "        return  gpt_logits, \\\n",
        "                bert_pool_toxic, \\\n",
        "                bert_pool_nontoxic, \\\n",
        "                toxic_reconstructed, \\\n",
        "                nontoxic_reconstructed, \\\n",
        "                toxic_latent_semantic, \\\n",
        "                nontoxic_latent_semantic, \\\n",
        "                toxic_latent_style, \\\n",
        "                self.nontoxic_latent_style\n",
        "        # Map the latent representations of toxic/non-toxic semantic/style\n",
        "\n",
        "\n",
        "\n",
        "        # toxic_semantic_latent = self.toxic_semantic(bert_pool_toxic)\n",
        "        # toxic_style_latent = self.toxic_style(bert_pool_toxic)\n",
        "\n",
        "        # # nontoxic_semantic_latent = self.toxic_semantic(bert_pool_nontoxic)\n",
        "        # self.nontoxic_style_latent = self.toxic_style(bert_pool_nontoxic)\n",
        "\n",
        "        # # This is the key step where we fuse the original semantic with non-toxic style\n",
        "        # fusion_latent = torch.cat((toxic_semantic_latent, self.nontoxic_style_latent),dim=1)\n",
        "        # fusion_latent = self.fusion(fusion_latent)\n",
        "\n",
        "        # Pass through some dense layers to generate tokens for GPT decoder\n",
        "        # h1 = self.hidden1(fusion_latent)\n",
        "        # h1 = nn.ReLU()(h1)\n",
        "        # h1 = nn.Dropout(0.2)(h1)\n",
        "\n",
        "        # h2 = self.hidden2(h1)\n",
        "        # h2 = nn.ReLU()(h2)\n",
        "        # h2 = nn.Dropout(0.2)(h2)\n",
        "        \n",
        "        # mapping = self.final_layer(h1)\n",
        "\n",
        "        # Mapping BERT vocab ids into GPT2 vocab ids\n",
        "        # mapping = self.mapping_hidden(mapping)\n",
        "        # mapping = nn.ReLU()(mapping)\n",
        "        # mapping = self.mapping_model(mapping)\n",
        "\n",
        "        # mapped_embeddings = self.gpt_model.transformer.wte(mapping.argmax(-1))\n",
        "        # mapping_repeated = mapped_embeddings.unsqueeze(1).repeat(1, 128, 1)\n",
        "        \n",
        "        # gpt_outputs = self.gpt_model(inputs_embeds=mapping_repeated, return_dict=True)\n",
        "        # gpt_logits = gpt_outputs.logits\n",
        "        \n",
        "        # return gpt_logits, (toxic_semantic_latent, toxic_style_latent, nontoxic_semantic_latent, self.nontoxic_style_latent)\n",
        "\n",
        "    def generate(self, toxic_input_ids, toxic_attention_mask, decoding_kwargs=None):\n",
        "        '''\n",
        "        Generating text based on toxic inputs and learned-nontoxic_style\n",
        "        '''\n",
        "        if not decoding_kwargs:\n",
        "            decoding_kwargs = {\n",
        "              \"num_return_sequences\": 1,\n",
        "              \"top_p\": 0.9,  # For nucleus sampling\n",
        "              \"top_k\": 50,  # For top-k sampling\n",
        "              \"temperature\": 0.8,  # For temperature sampling\n",
        "              \"do_sample\": True,  # Set to True for sampling-based methods\n",
        "              \"max_length\": 128,  # The maximum length of the generated text\n",
        "              \"eos_token_id\": self.gpt_model.config.eos_token_id\n",
        "          }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Encoding\n",
        "            toxic_bert = self.bert_model_toxic(input_ids=toxic_input_ids, attention_mask=toxic_attention_mask).last_hidden_state\n",
        "\n",
        "            # Grab the CLS token or the mean of last layer\n",
        "            bert_pool_toxic = toxic_bert.mean(dim=1)\n",
        "\n",
        "            toxic_latent_rep = self.toxic_encoder(bert_pool_toxic)\n",
        "            toxic_latent_semantic, toxic_latent_style = toxic_latent_rep[:, :2], toxic_latent_rep[:, 2:]\n",
        "\n",
        "            fusion_latent = torch.cat((toxic_latent_semantic, self.nontoxic_latent_style),dim=1)\n",
        "            fusion_latent = self.fusion(fusion_latent)\n",
        "\n",
        "            style_transfer_reconstructed = self.style_transfer_decoder(fusion_latent)\n",
        "\n",
        "            mapping = self.mapping_model(style_transfer_reconstructed)\n",
        "\n",
        "            mapped_embeddings = self.gpt_model.transformer.wte(mapping.argmax(-1))\n",
        "            mapping_repeated = mapped_embeddings.unsqueeze(1).repeat(1, 128, 1)\n",
        "\n",
        "            gpt_outputs = self.gpt_model.generate(inputs_embeds=mapping_repeated, **decoding_kwargs)\n",
        "\n",
        "        return gpt_outputs\n",
        "\n",
        "class StyleTransferLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StyleTransferLoss, self).__init__()\n",
        "    \n",
        "    def forward(self, \n",
        "                output,\n",
        "                target,\n",
        "                bert_pool_toxic, \n",
        "                bert_pool_nontoxic, \n",
        "                toxic_reconstructed, \n",
        "                nontoxic_reconstructed, \n",
        "                toxic_latent_semantic, \n",
        "                nontoxic_latent_semantic,\n",
        "                toxic_latent_style,\n",
        "                nontoxic_latent_style\n",
        "                ):\n",
        "\n",
        "        toxic_reconstruction_loss = F.mse_loss(toxic_reconstructed, bert_pool_toxic)\n",
        "        nontoxic_reconstruction_loss = F.mse_loss(nontoxic_reconstructed, bert_pool_nontoxic)\n",
        "        \n",
        "        semantic_loss = F.mse_loss(toxic_latent_semantic, nontoxic_latent_semantic)\n",
        "\n",
        "        # disentanglement_product = torch.matmul(toxic_latent_semantic, toxic_latent_style.t())\n",
        "        # disentanglement_frobenius_norm = torch.norm(disentanglement_product, p='fro')  # Compute the Frobenius norm of the product\n",
        "        # disentanglement_loss = disentanglement_frobenius_norm ** 2  # Square the Frobenius norm to get the loss\n",
        "\n",
        "        output = output.view(-1, output.size(-1))\n",
        "        target = target.view(-1)\n",
        "        token_loss = F.cross_entropy(output, target, ignore_index=gpt_tokenizer.pad_token_id)\n",
        "\n",
        "        total_loss = 5.* toxic_reconstruction_loss + \\\n",
        "                     5.* nontoxic_reconstruction_loss + \\\n",
        "                     semantic_loss + \\\n",
        "                     token_loss\n",
        "                    #  disentanglement_loss + \\\n",
        "        \n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "G2F8dTtOWNqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set up the training parameters\n",
        "batch_size = 8\n",
        "num_epochs = 20\n",
        "learning_rate = 1e-3\n",
        "\n",
        "bert_checkpoint = 'distilbert-base-uncased'\n",
        "gpt_checkpoint = 'gpt2'\n",
        "\n",
        "bert_tokenizer = DistilBertTokenizer.from_pretrained(bert_checkpoint)\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained(gpt_checkpoint)\n",
        "\n",
        "# Set up the data loaders\n",
        "train_dataset = TextStyleTransferDataset(data_path=train_file, \n",
        "                                         encoder_tokenizer = bert_tokenizer, \n",
        "                                         decoder_tokenizer = gpt_tokenizer,\n",
        "                                         max_len=128)\n",
        "eval_dataset = TextStyleTransferDataset(data_path=dev_file, \n",
        "                                        encoder_tokenizer = bert_tokenizer, \n",
        "                                        decoder_tokenizer = gpt_tokenizer,\n",
        "                                        max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set up the model\n",
        "model = TextStyleTransferModel()\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = StyleTransferLoss()\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    prog_bar = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "    prog_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for i, data in enumerate(prog_bar):\n",
        "        source_inputs = data['source_inputs']\n",
        "        gpt_inputs = data['gpt_inputs']\n",
        "        target_inputs = data['target_inputs']\n",
        "        target_gpt_inputs = data['target_gpt_inputs']\n",
        "\n",
        "        input_ids = source_inputs['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = source_inputs['attention_mask'].squeeze(1).to(device)\n",
        "        gpt_ids = gpt_inputs['input_ids'].squeeze(1).to(device)\n",
        "        # gpt_masks = gpt_inputs['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "        target_ids = target_inputs['input_ids'].squeeze(1).to(device)\n",
        "        target_attention_mask = target_inputs['attention_mask'].squeeze(1).to(device)\n",
        "        target_gpt_ids = target_gpt_inputs['input_ids'].squeeze(1).to(device)\n",
        "        # target_gpt_masks = target_gpt_inputs['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        gpt_output, bert_pool_toxic, bert_pool_nontoxic, toxic_reconstructed, nontoxic_reconstructed, toxic_latent_semantic, nontoxic_latent_semantic, toxic_latent_style, nontoxic_latent_style = model(input_ids, attention_mask, target_ids, target_attention_mask)\n",
        "\n",
        "        loss = criterion(gpt_output, \n",
        "                         target_gpt_ids, \n",
        "                         bert_pool_toxic, \n",
        "                         bert_pool_nontoxic, \n",
        "                         toxic_reconstructed, \n",
        "                         nontoxic_reconstructed, \n",
        "                         toxic_latent_semantic, \n",
        "                         nontoxic_latent_semantic,\n",
        "                         toxic_latent_style,\n",
        "                         nontoxic_latent_style\n",
        "                         )\n",
        "        \n",
        "        # Backpropagation and optimization step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        prog_bar.set_postfix(loss=running_loss/(i+1))\n",
        "    \n",
        "    losses.append(running_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNE_9YjLkmxu",
        "outputId": "64b1ef59-ac5b-4893-f108-232011cd2d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Epoch 1/20: 100%|██████████| 199/199 [00:30<00:00,  6.58it/s, loss=7.39]\n",
            "Epoch 2/20: 100%|██████████| 199/199 [00:28<00:00,  6.88it/s, loss=6.8]\n",
            "Epoch 3/20: 100%|██████████| 199/199 [00:28<00:00,  6.88it/s, loss=6.66]\n",
            "Epoch 4/20: 100%|██████████| 199/199 [00:28<00:00,  6.87it/s, loss=6.6]\n",
            "Epoch 5/20: 100%|██████████| 199/199 [00:28<00:00,  6.87it/s, loss=6.56]\n",
            "Epoch 6/20: 100%|██████████| 199/199 [00:28<00:00,  6.87it/s, loss=6.52]\n",
            "Epoch 7/20: 100%|██████████| 199/199 [00:28<00:00,  6.88it/s, loss=6.47]\n",
            "Epoch 8/20: 100%|██████████| 199/199 [00:28<00:00,  6.86it/s, loss=6.45]\n",
            "Epoch 9/20: 100%|██████████| 199/199 [00:28<00:00,  6.87it/s, loss=6.42]\n",
            "Epoch 10/20: 100%|██████████| 199/199 [00:28<00:00,  6.87it/s, loss=6.39]\n",
            "Epoch 11/20: 100%|██████████| 199/199 [00:28<00:00,  6.88it/s, loss=6.36]\n",
            "Epoch 12/20: 100%|██████████| 199/199 [00:28<00:00,  6.88it/s, loss=6.33]\n",
            "Epoch 13/20: 100%|██████████| 199/199 [00:28<00:00,  6.87it/s, loss=6.31]\n",
            "Epoch 14/20: 100%|██████████| 199/199 [00:29<00:00,  6.85it/s, loss=6.28]\n",
            "Epoch 15/20: 100%|██████████| 199/199 [00:29<00:00,  6.86it/s, loss=6.25]\n",
            "Epoch 16/20: 100%|██████████| 199/199 [00:28<00:00,  6.88it/s, loss=6.25]\n",
            "Epoch 17/20: 100%|██████████| 199/199 [00:28<00:00,  6.87it/s, loss=6.23]\n",
            "Epoch 18/20: 100%|██████████| 199/199 [00:28<00:00,  6.88it/s, loss=6.21]\n",
            "Epoch 19/20: 100%|██████████| 199/199 [00:28<00:00,  6.87it/s, loss=6.19]\n",
            "Epoch 20/20: 100%|██████████| 199/199 [00:28<00:00,  6.88it/s, loss=6.16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"bitch ass get out of here you don't belong\"\n",
        "test_inputs = gpt_tokenizer(test_text)\n",
        "test_input_id = test_inputs['input_ids']\n",
        "test_input_mask = test_inputs['attention_mask']\n",
        "test_input_id = torch.tensor(test_input_id).unsqueeze(0).to(device)\n",
        "test_input_mask = torch.tensor(test_input_mask).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "zq_16YpYJOiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoding_kwargs = {\n",
        "              \"num_return_sequences\": 1,\n",
        "              \"top_p\": 0.9,  # For nucleus sampling\n",
        "              \"top_k\": 50,  # For top-k sampling\n",
        "              \"temperature\": 0.8,  # For temperature sampling\n",
        "              \"do_sample\": True,  # Set to True for sampling-based methods\n",
        "              \"max_length\": 128,  # The maximum length of the generated text\n",
        "              \"eos_token_id\": model.gpt_model.config.eos_token_id\n",
        "          }"
      ],
      "metadata": {
        "id": "vYHEhfIzM4eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(test_input_id, test_input_mask)"
      ],
      "metadata": {
        "id": "kf_JVJjNPi0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe08ebd-2822-4400-f6be-ee0a5f3528bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[50256,    13,    13,    13,    13,    30,    13,    13,    13,    13,\n",
              "           407,  2089,   329,   257,   318,   318,   257,   318,   340,   470,\n",
              "           318,   470,   257,   257,   345,   340,    13,    13,    13,    13,\n",
              "            13,    13,    13,    13,    13,    13,    13,    13,    13,    13,\n",
              "            13,    13,    13,   986,    13,    30,    13,    13,    13,    13,\n",
              "            13,    13,    13,    30,    13,    30,    13,    13,    13,    13,\n",
              "             0,    13,    13,    13,    13,    13,    13,    13,     0,    13,\n",
              "            13,    13,    30,    13,  1639,   284,   389,   287,   345,    11,\n",
              "            13,    40,    13,    13,  5812,   257,   345,   651,   389,   262,\n",
              "           319,   345,    13,   407,   389,   821,   257,   470,    11,   284,\n",
              "           447,   588,   284,  1639,    13,   338,   284,    11,    11,   470,\n",
              "           470,   345,   318,  5812,   284,   284,   470,   257,   257,   319,\n",
              "           470,   326,    13,   407,  1639,  1212,    11,   345]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_tokenizer.batch_decode(model.generate(test_input_id, toxic_attention_mask=None), skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "sJNY6rYtJkuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93adc3af-d00f-4a95-a03f-02b2b23f984b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\".. it to you think up it to just't up't you a to of? are you. you not for for....?....?................. it you is like a a a the.?. that you is the of's so of. people like I and're to,, so is with it're. people be get theyYou for't the be people,?? get't and are a a a to are, don to,'s this are is YouNo to you, you are of and you\"]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0aQtKy0rXuUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZdlzsXtXuSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ua0fHDviXuPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc33r6nqIJ3D",
        "outputId": "50ce8cdf-867a-4a24-82d2-404a0084c1e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model.generate(inputs_embeds=test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "rmvoJQJXH5FV",
        "outputId": "79f13b5e-a2a7-4114-f813-1040a9ebfbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-791fb9beae47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2200\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2201\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2202\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50257) must match the size of tensor b (768) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "BDBA-GxIID5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_latent_dim = 8\n",
        "style_latent_dim = 4\n",
        "fusion_dim = 32\n",
        "\n",
        "bert_model_toxic = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "bert_model_nontoxic = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "toxic_semantic = nn.Linear(in_features = bert_model_toxic.config.hidden_size,\n",
        "                                out_features = semantic_latent_dim,\n",
        "                                bias=True)\n",
        "toxic_style = nn.Linear(in_features = bert_model_toxic.config.hidden_size,\n",
        "                              out_features = style_latent_dim,\n",
        "                              bias=True)\n",
        "\n",
        "nontoxic_semantic = nn.Linear(in_features = bert_model_nontoxic.config.hidden_size,\n",
        "                                    out_features = semantic_latent_dim,\n",
        "                                    bias=True)\n",
        "nontoxic_style = nn.Linear(in_features = bert_model_nontoxic.config.hidden_size,\n",
        "                                out_features = style_latent_dim,\n",
        "                                bias=True)\n",
        "\n",
        "fusion = nn.Linear(in_features = semantic_latent_dim + style_latent_dim,\n",
        "                        out_features = fusion_dim,\n",
        "                        bias=True)\n",
        "\n",
        "hidden1 = nn.Linear(fusion_dim, int(5*fusion_dim))\n",
        "# self.hidden2 = nn.Linear(self.hidden1.in_features, int(5*self.hidden1.out_features))\n",
        "\n",
        "final_layer = nn.Linear(in_features = hidden1.out_features,\n",
        "                              out_features = bert_model_toxic.config.vocab_size,\n",
        "                              bias=True)\n",
        "\n",
        "# mapping_hidden = nn.Linear(bert_model_toxic.config.vocab_size, gpt_model.config.vocab_size).to(device)\n",
        "mapping_model = nn.Linear(bert_model_toxic.config.vocab_size, gpt_model.config.vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W_pBfBNAwby",
        "outputId": "c7893f5e-2e6c-4cc5-d96c-de16d52621eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding\n",
        "toxic_bert = bert_model_toxic(input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "nontoxic_bert = bert_model_nontoxic(target_ids, attention_mask=target_attention_mask).last_hidden_state\n",
        "\n",
        "# Grab the CLS token or the mean of last layer\n",
        "bert_pool_toxic = toxic_bert.mean(dim=1)\n",
        "bert_pool_nontoxic = nontoxic_bert.mean(dim=1)\n",
        "# content_latent = content_outputs[0]\n",
        "\n",
        "# Map the latent representations of toxic/non-toxic semantic/style\n",
        "toxic_semantic_latent = toxic_semantic(bert_pool_toxic)\n",
        "toxic_style_latent = toxic_style(bert_pool_toxic)\n",
        "\n",
        "nontoxic_semantic_latent = toxic_semantic(bert_pool_nontoxic)\n",
        "nontoxic_style_latent = toxic_style(bert_pool_nontoxic)\n",
        "\n",
        "# This is the key step where we fuse the original semantic with non-toxic style\n",
        "fusion_latent = torch.cat((toxic_semantic_latent, nontoxic_style_latent),dim=1)\n",
        "fusion_latent = fusion(fusion_latent)\n",
        "\n",
        "# Pass through some dense layers to generate tokens for GPT decoder\n",
        "h1 = hidden1(fusion_latent)\n",
        "h1 = nn.ReLU()(h1)\n",
        "h1 = nn.Dropout(0.2)(h1)\n",
        "\n",
        "# h2 = self.hidden2(h1)\n",
        "# h2 = nn.ReLU()(h2)\n",
        "# h2 = nn.Dropout(0.2)(h2)\n",
        "\n",
        "mapping = final_layer(h1)\n",
        "\n",
        "# Mapping BERT vocab ids into GPT2 vocab ids\n",
        "# mapping = mapping_hidden(mapping)\n",
        "mapping = nn.ReLU()(mapping)\n",
        "mapping = mapping_model(mapping)\n",
        "\n",
        "mapped_embeddings = gpt_model.transformer.wte(mapping.argmax(-1))\n",
        "mapping_repeated = mapped_embeddings.unsqueeze(1).repeat(1, 128, 1)\n",
        "\n",
        "gpt_outputs = gpt_model(inputs_embeds=mapping_repeated, return_dict=True)\n",
        "gpt_logits = gpt_outputs.logits\n",
        "\n"
      ],
      "metadata": {
        "id": "Fqlab4WDA8ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model.transformer.wte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNfHVNi0DD2v",
        "outputId": "683817b0-0e63-48f0-ef6b-3829419ad36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model.transformer.wte(mapping.argmax(-1)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UapTK4WzCo9t",
        "outputId": "a2f38edd-e136-4c8d-8fec-468e81e6a3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIgMniB_AwZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LUO9U-DAwWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_PpkyiZAwO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "device = 'cpu'\n",
        "train_dataset = TextStyleTransferDataset(data_path=train_file, \n",
        "                                         encoder_tokenizer = bert_tokenizer,\n",
        "                                         decoder_tokenizer = gpt2_tokenizer, \n",
        "                                         max_len=128)\n",
        "eval_dataset = TextStyleTransferDataset(data_path=dev_file, \n",
        "                                        encoder_tokenizer = bert_tokenizer,\n",
        "                                        decoder_tokenizer = gpt2_tokenizer,\n",
        "                                        max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "for data in train_loader:\n",
        "    source_inputs = data['source_inputs']\n",
        "    target_inputs = data['target_inputs']\n",
        "    gpt_inputs = data['gpt_inputs']\n",
        "    target_gpt_inputs = data['target_gpt_inputs']\n",
        "\n",
        "    input_ids = source_inputs['input_ids'].squeeze(1).to(device)\n",
        "    attention_mask = source_inputs['attention_mask'].squeeze(1).to(device)\n",
        "    gpt_ids = gpt_inputs['input_ids'].squeeze(1).to(device)\n",
        "    gpt_masks = gpt_inputs['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "    target_ids = target_inputs['input_ids'].squeeze(1).to(device)\n",
        "    target_attention_mask = target_inputs['attention_mask'].squeeze(1).to(device)\n",
        "    target_gpt_ids = target_gpt_inputs['input_ids'].squeeze(1).to(device)\n",
        "    target_gpt_masks = target_gpt_inputs['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "W6fASMqWaylK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_latent_dim = 8\n",
        "style_latent_dim = 4\n",
        "fusion_dim = 32\n",
        "\n",
        "bert_model_toxic = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "bert_model_nontoxic = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "toxic_semantic = nn.Linear(in_features = bert_model_toxic.config.hidden_size,\n",
        "                                out_features = semantic_latent_dim,\n",
        "                                bias=True).to(device)\n",
        "toxic_style = nn.Linear(in_features = bert_model_toxic.config.hidden_size,\n",
        "                                out_features = style_latent_dim,\n",
        "                                bias=True).to(device)\n",
        "\n",
        "nontoxic_semantic = nn.Linear(in_features = bert_model_nontoxic.config.hidden_size,\n",
        "                                    out_features = semantic_latent_dim,\n",
        "                                    bias=True).to(device)\n",
        "nontoxic_style = nn.Linear(in_features = bert_model_nontoxic.config.hidden_size,\n",
        "                                out_features = style_latent_dim,\n",
        "                                bias=True).to(device)\n",
        "\n",
        "fusion = nn.Linear(in_features = semantic_latent_dim + style_latent_dim,\n",
        "                out_features = fusion_dim,\n",
        "                bias=True).to(device)\n",
        "\n",
        "hidden1 = nn.Linear(fusion_dim, int(5*fusion_dim)).to(device)\n",
        "\n",
        "final_layer = nn.Linear(in_features = hidden1.out_features,\n",
        "                                out_features = bert_model_toxic.config.vocab_size,\n",
        "                                bias=True).to(device)\n",
        "\n",
        "# mapping_hidden = nn.Linear(bert_model_toxic.config.vocab_size, gpt_model.config.vocab_size)\n",
        "mapping_model = nn.Linear(bert_model_toxic.config.vocab_size, gpt_model.config.n_embd).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VC7sJnGghDL",
        "outputId": "16b090eb-8174-4ce9-bc2d-962c6718da90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoding_kwargs = {\n",
        "    \"num_return_sequences\": 1,\n",
        "    \"top_p\": 0.9,  # For nucleus sampling\n",
        "    \"top_k\": 50,  # For top-k sampling\n",
        "    \"temperature\": 0.8,  # For temperature sampling\n",
        "    \"do_sample\": True,  # Set to True for sampling-based methods\n",
        "    \"max_length\": 128,  # The maximum length of the generated text\n",
        "    \"eos_token_id\": gpt_model.config.eos_token_id\n",
        "}"
      ],
      "metadata": {
        "id": "kDaSeKK5yEGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_bert = bert_model_toxic(input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "nontoxic_bert = bert_model_nontoxic(target_ids, attention_mask=target_attention_mask).last_hidden_state\n",
        "\n",
        "# Grab the CLS token or the mean of last layer\n",
        "bert_pool_toxic = toxic_bert.mean(dim=1)\n",
        "bert_pool_nontoxic = nontoxic_bert.mean(dim=1)\n",
        "# content_latent = content_outputs[0]\n",
        "\n",
        "# Map the latent representations of toxic/non-toxic semantic/style\n",
        "toxic_semantic_latent = toxic_semantic(bert_pool_toxic)\n",
        "toxic_style_latent = toxic_style(bert_pool_toxic)\n",
        "\n",
        "nontoxic_semantic_latent = nontoxic_semantic(bert_pool_nontoxic)\n",
        "nontoxic_style_latent = nontoxic_style(bert_pool_nontoxic)\n",
        "\n",
        "# This is the key step where we fuse the original semantic with non-toxic style\n",
        "fusion_latent = torch.cat((toxic_semantic_latent, nontoxic_style_latent),dim=1)\n",
        "fusion_latent = fusion(fusion_latent)\n",
        "\n",
        "# Pass through some dense layers to generate tokens for GPT decoder\n",
        "h1 = hidden1(fusion_latent)\n",
        "h1 = nn.ReLU()(h1)\n",
        "h1 = nn.Dropout(0.2)(h1)\n",
        "\n",
        "# h2 = self.hidden2(h1)\n",
        "# h2 = nn.ReLU()(h2)\n",
        "# h2 = nn.Dropout(0.2)(h2)\n",
        "\n",
        "mapping = final_layer(h1)\n",
        "\n",
        "# Mapping BERT vocab ids into GPT2 vocab ids\n",
        "# f = mapping_hidden(mapping)\n",
        "# f = nn.ReLU()(f)\n",
        "f = mapping_model(mapping)"
      ],
      "metadata": {
        "id": "-DFZTr-Hgg-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_repeated = f.unsqueeze(1).repeat(1, 128, 1)\n",
        "\n",
        "gpt_outputs = gpt_model(inputs_embeds=mapping_repeated, return_dict=True)\n",
        "gpt_logits = gpt_outputs.logits\n",
        "\n",
        "# output_tokens = gpt_model.generate(input_ids=None, input_embeds=mapping, **decoding_kwargs)\n"
      ],
      "metadata": {
        "id": "3R2bCoqpyPqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzQpQ0f44BUK",
        "outputId": "ab6da223-2f2b-4f8e-b303-e953b7a899d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 128, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeOLGZ3o4Dwr",
        "outputId": "54c34e60-ebca-4f20-8867-5d83a4da71bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxZCimC34HRm",
        "outputId": "25e74721-1da0-449d-fc9e-ba06ca297b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_logits = gpt_logits.view(-1, gpt_logits.size(-1))\n",
        "target = target_gpt_ids.view(-1)\n",
        "F.cross_entropy(output_logits, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "h694x1Zh149-",
        "outputId": "a37b9613-cee1-43f3-d810-6acb24293134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-999684d27e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_gpt_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_gpt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 50257], got [4, 128]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA0xanXL3ZgL",
        "outputId": "7242c8cf-b600-4ce1-9ab1-719c2a21b467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0346, device='cuda:0', grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "TdsaTISnmcvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model.to('cpu')\n",
        "f = f.to('cpu')\n",
        "gpt_model.generate(inputs_embeds = f[0][0],\n",
        "                   max_length = 128,\n",
        "                   num_beams=5,\n",
        "                   top_p = 0.93,\n",
        "                   do_sample=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "3GYVgMYvgg79",
        "outputId": "2729dada-3195-4d88-bcdd-56c154db9838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-91d14373a7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m gpt_model.generate(inputs_embeds = f[0][0],\n\u001b[0m\u001b[1;32m      4\u001b[0m                    \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1256\u001b[0m             if (\n\u001b[1;32m   1257\u001b[0m                 \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m                 \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             ):\n\u001b[1;32m   1260\u001b[0m                 logger.warning(\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(f, dim=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odpzknCjDvqc",
        "outputId": "b02f9708-90e1-4a74-dbc2-b6ff57e291c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[12647, 12647, 12647,  ..., 12647, 12647, 12647],\n",
              "        [19962, 19962, 19962,  ..., 19962, 19962, 19962],\n",
              "        [30890, 30890, 30890,  ..., 30890, 30890, 30890],\n",
              "        ...,\n",
              "        [ 4384,  4384,  4384,  ...,  4384,  4384,  4384],\n",
              "        [19962, 19962, 19962,  ..., 19962, 19962, 19962],\n",
              "        [ 4384,  4384,  4384,  ...,  4384,  4384,  4384]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_latent_dim = 128\n",
        "style_latent_dim = 8\n",
        "fusion_dim = 256\n",
        "\n",
        "bert_model_toxic = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "toxic_semantic = nn.Linear(bert_model_toxic.config.hidden_size, semantic_latent_dim).to(device)\n",
        "toxic_style = nn.Linear(bert_model_toxic.config.hidden_size, style_latent_dim).to(device)\n",
        "\n",
        "bert_model_nontoxic = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "nontoxic_semantic = nn.Linear(bert_model_nontoxic.config.hidden_size, semantic_latent_dim).to(device)\n",
        "nontoxic_style = nn.Linear(bert_model_nontoxic.config.hidden_size, style_latent_dim).to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n_m5DGCbvRH",
        "outputId": "def69dd1-9910-423c-b563-bfa4e158fea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fusion = nn.Linear(in_features = semantic_latent_dim + style_latent_dim,\n",
        "                        out_features = fusion_dim,\n",
        "                        bias=True).to(device)\n",
        "\n",
        "hidden1 = nn.Linear(fusion_dim, int(1.5*fusion_dim)).to(device)\n",
        "hidden2 = nn.Linear(hidden1.in_features, int(2*fusion_dim)).to(device)\n",
        "\n",
        "final_layer = nn.Linear(in_features = hidden2.out_features,\n",
        "                                out_features = bert_model_toxic.config.vocab_size,\n",
        "                                bias=True).to(device)"
      ],
      "metadata": {
        "id": "qNPsxwjI2Ex8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "input_dim = bert_model_toxic.config.vocab_size\n",
        "output_dim = gpt2_model.config.vocab_size\n",
        "\n",
        "mapping_model = MappingModel(input_dim, output_dim).to(device)"
      ],
      "metadata": {
        "id": "v74iL9lx0_kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_bert = bert_model_toxic(input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "toxic_pool = toxic_bert.mean(dim=1)\n",
        "toxic_latent_semantic = toxic_semantic(toxic_pool)\n",
        "toxic_latent_style = toxic_style(toxic_pool)\n",
        "\n",
        "nontoxic_bert = bert_model_nontoxic(target_ids, attention_mask=target_attention_mask).last_hidden_state\n",
        "nontoxic_pool = nontoxic_bert.mean(dim=1)\n",
        "nontoxic_latent_semantic = toxic_semantic(nontoxic_pool)\n",
        "nontoxic_latent_style = nontoxic_style(nontoxic_pool)"
      ],
      "metadata": {
        "id": "asLa31q-bItC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fusion_latent = torch.cat((nontoxic_latent_semantic, nontoxic_latent_style),dim=1)\n",
        "fusion_latent = fusion(fusion_latent)\n",
        "h1 = hidden1(fusion_latent)\n",
        "h1 = nn.ReLU()(h1)\n",
        "h1 = nn.Dropout(0.2)(fusion_latent)\n",
        "h2 = hidden2(h1)\n",
        "h2 = nn.ReLU()(h2)\n",
        "h2 = nn.Dropout(0.2)(h2)\n",
        "f = final_layer(h2)\n",
        "f = mapping_model(f)\n",
        "f = f.unsqueeze(1).repeat(1, input_ids.shape[1], 1)\n",
        "out = torch.argmax(f, dim=2)"
      ],
      "metadata": {
        "id": "4eXTJt0by1gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)"
      ],
      "metadata": {
        "id": "yr5sy-bIzrTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.MSELoss()(out*target_gpt_masks, target_gpt_ids*target_gpt_masks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "wm_GgcXt_mx0",
        "outputId": "3fce5d43-7bdc-4033-a9b9-4c356447928a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-30cb69ba78cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtarget_gpt_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_gpt_ids\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtarget_gpt_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"mse_cuda\" not implemented for 'Long'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEXT: IMPLEMENT LOSS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wngWJo9gAGqC",
        "outputId": "6662bc8f-9820-4831-84b2-46a9e75ab7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulz6jkO3_Kwf",
        "outputId": "3b29c115-ebd7-4dd8-af4c-ad58b3e6179b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(params=model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "A7OVfDbfhQO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextStyleTransferModel(128, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmcP7HLdhgux",
        "outputId": "fbb20e3e-2160-4265-ae32-085f7ed3ea51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_semantic_latent, toxic_style_latent, nontoxic_semantic_latent, nontoxic_style_latent = model(input_ids, attention_mask, target_ids, target_attention_mask)"
      ],
      "metadata": {
        "id": "YaF7a43uhqiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(mse, source_semantic, source_style, target_semantic, target_style, lambda_semantic=2.0, lambda_style=1.0):\n",
        "\n",
        "    return lambda_semantic * mse(source_semantic, target_semantic) - lambda_style * mse(source_style, target_style)"
      ],
      "metadata": {
        "id": "ZVnnwr3OjXtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = nn.MSELoss()\n",
        "loss = loss_fn(mse, toxic_latent_semantic, nontoxic_latent_semantic, toxic_latent_style, nontoxic_latent_semantic, 8, 1) "
      ],
      "metadata": {
        "id": "yItDSRFFiG8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m43evusAjqdL",
        "outputId": "d8b12d09-ad2d-444e-ec18-d3596427d239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0., grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "DQwfXjHVb2OZ",
        "outputId": "c2435e2b-e015-43e1-c8cf-4c4b1c0cd4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-223fecb93de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIIeC2JDgZ-A",
        "outputId": "2edc776d-a2d9-4dde-99ec-8910bb95ac4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set up the training parameters\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "bert_checkpoint = 'distilbert-base-uncased'\n",
        "gpt_checkpoint = 'gpt2'\n",
        "\n",
        "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Set up the data loaders\n",
        "train_dataset = TextStyleTransferDataset(data_path=train_file, \n",
        "                                         tokenizer = bert_tokenizer, \n",
        "                                         max_len=128)\n",
        "eval_dataset = TextStyleTransferDataset(data_path=dev_file, \n",
        "                                        tokenizer = bert_tokenizer,\n",
        "                                         max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for batch in train_loader:\n",
        "    offensive_texts, neutralized_texts = batch\n",
        "\n",
        "    input_texts = offensive_texts\n",
        "    input_ids = batch['source_inputs']['input_ids'].squeeze(1).to(device)\n",
        "    attention_mask = batch['source_inputs']['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "    target_texts = neutralized_texts\n",
        "    target_ids = batch['target_inputs']['input_ids'].squeeze(1).to(device)\n",
        "    target_attention_mask = batch['target_inputs']['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "    break\n",
        "    \n",
        "\n",
        "content_dim = 256\n",
        "style_dim = 256\n",
        "\n",
        "bert_model = DistilBertModel.from_pretrained(bert_checkpoint).to(device)\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(gpt_checkpoint).to(device)\n",
        "\n",
        "\n",
        "content_outputs = self.bert_model(content_input_ids, attention_mask=content_attention_mask).last_hidden_state\n",
        "content_latent = content_outputs.mean(dim=1)\n",
        "# content_latent = self.content_projection(content_outputs.mean(dim=1))\n",
        "\n",
        "if style_input_ids is not None and style_attention_mask is not None:\n",
        "    style_outputs = self.bert_model(style_input_ids, attention_mask=style_attention_mask).last_hidden_state\n",
        "\n",
        "    style_embedding = self.style_encoder(content_latent) + self.style_decoder(style_latent)\n",
        "    # style_latent = self.style_projection(style_outputs.mean(dim=1))\n",
        "\n",
        "else:\n",
        "    style_embedding = style_embeddings\n",
        "\n",
        "# Concatenate content and style embeddings and fuse them\n",
        "fused_latent = torch.cat((content_latent, style_embedding), dim=1)\n",
        "fused_latent = self.fusion(fused_latent)\n",
        "\n",
        "gpt_input = fused_latent.unsqueeze(1).repeat(1, content_input_ids.shape[1], 1)\n",
        "gpt_output = self.gpt_model(inputs_embeds=gpt_input, attention_mask=content_attention_mask)\n",
        "\n",
        "gpt_logits = gpt_output.logits"
      ],
      "metadata": {
        "id": "ad_TiO5okdHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# content_dim = 256\n",
        "# style_dim = 256\n",
        "\n",
        "# bert_model = DistilBertModel.from_pretrained(bert_checkpoint).to(device)\n",
        "# gpt_model = GPT2LMHeadModel.from_pretrained(gpt_checkpoint).to(device)\n",
        "\n",
        "# style_vector = None\n",
        "# # Content Encoder\n",
        "# content_encoder = nn.Sequential(\n",
        "#     nn.Linear(bert_model.config.hidden_size, content_dim),\n",
        "#     nn.ELU()\n",
        "# ).to(device)\n",
        "\n",
        "# # Style Encoder\n",
        "# style_encoder = nn.Sequential(\n",
        "#     nn.Linear(bert_model.config.hidden_size, style_dim),\n",
        "#     nn.ELU()\n",
        "# ).to(device)\n",
        "\n",
        "# # Decoder\n",
        "# decoder = nn.Linear(gpt_model.config.n_embd, gpt_model.config.vocab_size, bias=False)\n",
        "\n",
        "# content_outputs = bert_model(content_inputs, attention_mask=content_attention_mask).last_hidden_state\n",
        "# content_latent = content_encoder(content_outputs.mean(dim=1))\n",
        "\n",
        "# fusion = nn.Linear(content_dim + style_dim, gpt_model.config.n_embd).to(device)\n",
        "# if style_vector is None:\n",
        "#     style_outputs = bert_model(style_inputs, attention_mask=style_attention_mask).last_hidden_state\n",
        "#     style_latent = style_encoder(style_outputs.mean(dim=1))\n",
        "# else:\n",
        "#     style_latent = style_vector\n",
        "\n",
        "# # Decoding\n",
        "# fused_latent = torch.cat((content_latent, style_latent), dim=1)\n",
        "# fused_latent = fusion(fused_latent)\n",
        "# gpt_input = fused_latent.unsqueeze(1).repeat(1, content_inputs.shape[1], 1)\n",
        "# gpt_output = gpt_model(inputs_embeds=gpt_input)"
      ],
      "metadata": {
        "id": "BwS3ACIBWUDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "K9B2olOMXGie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_test = model.bert_model(input_ids, attention_mask)"
      ],
      "metadata": {
        "id": "oPmu5sJH1jpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_test.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u94iGFnN1rtJ",
        "outputId": "01688b57-eb3c-43e7-a9b9-4b723031d2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set up the training parameters\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "bert_checkpoint = 'distilbert-base-uncased'\n",
        "\n",
        "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Set up the data loaders\n",
        "train_dataset = TextStyleTransferDataset(data_path=train_file, \n",
        "                                         tokenizer = bert_tokenizer, \n",
        "                                         max_len=128)\n",
        "eval_dataset = TextStyleTransferDataset(data_path=dev_file, \n",
        "                                        tokenizer = bert_tokenizer,\n",
        "                                         max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set up the model\n",
        "# bert_model = DistilBertModel.from_pretrained(bert_checkpoint)\n",
        "# gpt_model = GPT2LMHeadModel.from_pretrained(gpt_checkpoint)\n",
        "model = TextStyleTransferModel(bert_model, gpt_model)\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    prog_bar = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "    prog_bar.set_description(f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for batch in prog_bar:\n",
        "        offensive_texts, neutralized_texts = batch\n",
        "\n",
        "        input_texts = offensive_texts\n",
        "        input_ids = batch['source_inputs']['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = batch['source_inputs']['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "        target_texts = neutralized_texts\n",
        "        target_ids = batch['target_inputs']['input_ids'].squeeze(1).to(device)\n",
        "        target_attention_mask = batch['target_inputs']['attention_mask'].squeeze(1).to(device)\n",
        "        # target_ids = tokenizer.batch_encode_plus(target_texts, padding=True, truncation=True, return_tensors='pt')['input_ids']\n",
        "        # target_attention_mask = (target_ids != tokenizer.pad_token_id)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, style_embedding = model(input_ids, attention_mask, target_ids, target_attention_mask)\n",
        "\n",
        "        # # Generate text and compute loss\n",
        "        # logits, generated_style = model(input_ids, attention_mask)\n",
        "        # target_logits, target_style = model(target_ids, target_attention_mask)\n",
        "\n",
        "        # loss = style_transfer_loss(\n",
        "        #     generated_style, target_style,\n",
        "        #     logits.view(-1, logits.shape[-1]), target_logits.view(-1, target_logits.shape[-1]),\n",
        "        #     lambda_style=1.0, lambda_logits=1.0\n",
        "        # )\n",
        "\n",
        "        # Backpropagation and optimization step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    prog_bar.set_postfix(loss=running_loss/len(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "Q5hbNSg9zwC7",
        "outputId": "81fcd978-466f-409e-d89d-9633ad355a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Epoch 1:   0%|          | 0/397 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-432a6c8466fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Generate text and compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mtarget_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-e941de13294d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mbert_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mbert_pooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Project the hidden states into the style embedding space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutput' object has no attribute 'pooler_output'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set up the training parameters\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "bert_checkpoint = 'distilbert-base-uncased'\n",
        "gpt_checkpoint = 'gpt2'\n",
        "\n",
        "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Set up the data loaders\n",
        "train_dataset = TextStyleTransferDataset(data_path=train_file, \n",
        "                                         tokenizer = bert_tokenizer, \n",
        "                                         max_len=128)\n",
        "eval_dataset = TextStyleTransferDataset(data_path=dev_file, \n",
        "                                        tokenizer = bert_tokenizer,\n",
        "                                         max_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set up the model\n",
        "bert_model = DistilBertModel.from_pretrained(bert_checkpoint)\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(gpt_checkpoint)\n",
        "model = TextStyleTransferModel(256, 256, bert_model, gpt_model)\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    style_embeddings = []\n",
        "\n",
        "    model.train()\n",
        "    prog_bar = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "    prog_bar.set_description(f\"Epoch {epoch+1}\")\n",
        "    for i, data in enumerate(prog_bar):\n",
        "\n",
        "        source_input_ids = data['source_inputs']['input_ids'].squeeze(1).to(device)\n",
        "        source_attention_mask = data['source_inputs']['attention_mask'].squeeze(1).to(device)\n",
        "        target_input_ids = data['target_inputs']['input_ids'].squeeze(1).to(device)\n",
        "        target_attention_mask = data['target_inputs']['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        gpt_logits, style_embedding = model(source_input_ids, source_attention_mask, target_input_ids, target_attention_mask)\n",
        "        \n",
        "        target_logits = target_input_ids.view(-1)\n",
        "\n",
        "        # loss = style_transfer_loss(gpt_logits, source_embedding, target_embedding, )\n",
        "\n",
        "        loss = criterion(gpt_logits.view(-1, gpt_model.config.vocab_size), target_logits)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        prog_bar.set_postfix(loss=loss)\n",
        "\n",
        "    style_embedding = style_embedding.mean(dim=0).detach().to('cpu').numpy()\n",
        "    style_embeddings.append(style_embedding)\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    eval_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(eval_loader):\n",
        "            content_inputs = data['content_input']['input_ids'].squeeze(1).to(device)\n",
        "            content_attention_mask = data['content_input']['attention_mask'].squeeze(1).to(device)\n",
        "            style_inputs = data['style_input']['input_ids'].squeeze(1).to(device)\n",
        "            style_attention_mask = data['style_input']['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "            outputs = model(content_inputs, content_attention_mask, style_inputs, style_attention_mask)\n",
        "            loss = criterion(outputs.view(-1, gpt_model.config.vocab_size), style_inputs.view(-1))\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 2)\n",
        "            eval_acc += (predicted == style_inputs).sum().item()\n",
        "\n",
        "    print('Epoch %d train loss: %.3f eval loss: %.3f eval acc: %.3f' % (epoch + 1, running_loss / len(train_loader), eval_loss / len(eval_loader), eval_acc / (len(eval_dataset) * 128)), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "PByWasbkt0XI",
        "outputId": "3135ea16-216b-4a53-f8f4-f7f78c8b8d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Epoch 1:   0%|          | 0/397 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-775b8965bc2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mgpt_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtarget_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-20b8c4424660>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, content_input_ids, content_attention_mask, style_input_ids, style_attention_mask, style_embeddings)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Concatenate content and style embeddings and fuse them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mfused_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mfused_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 1 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "DnsRAFMei287"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved model\n",
        "model = TextStyleTransferModel(content_dim=256, style_dim=256, bert_model=bert_model, gpt_model=gpt_model)\n",
        "model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# input offensive text\n",
        "offensive_text = \"I hate this place so much. The service is terrible and the food is disgusting.\"\n",
        "\n",
        "# tokenize offensive text\n",
        "offensive_tokens = tokenizer(offensive_text, max_length=max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
        "offensive_inputs = offensive_tokens.input_ids.to(device)\n",
        "offensive_attention_mask = offensive_tokens.attention_mask.to(device)\n",
        "\n",
        "# generate neutralized text\n",
        "with torch.no_grad():\n",
        "    _, offensive_embedding = model.bert_model(offensive_inputs, attention_mask=offensive_attention_mask)\n",
        "    neutralized_embedding = model.style_embedding(offensive_embedding)\n",
        "    generated_ids = model.decoder.generate(neutralized_embedding, max_length=max_len, do_sample=True)\n",
        "    neutralized_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    \n",
        "# print neutralized text\n",
        "print(neutralized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uUAwAVDi6hx",
        "outputId": "8aee7de0-a662-4a4e-9414-6e4bd577aadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=768, out_features=256, bias=True)\n",
              "  (1): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(style_embeddings[-1]).mean(dim=0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-81DrkOCsI33",
        "outputId": "75a21eb1-18cc-4ec8-962f-3edfba68eefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([768])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextStyleTransferModelFailed1(nn.Module):\n",
        "    def __init__(self, content_dim, style_dim, bert_model, gpt_model):\n",
        "        super(TextStyleTransferModel, self).__init__()\n",
        "        \n",
        "        self.bert_model = bert_model\n",
        "        self.gpt_model = gpt_model\n",
        "\n",
        "        self.style_encoder = nn.Linear(self.bert_model.config.hidden_size, self.bert_model.config.hidden_size)\n",
        "        self.style_decoder = nn.Linear(self.bert_model.config.hidden_size, self.bert_model.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Encode the input using DistilBERT\n",
        "        bert_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        bert_hidden_states = bert_output.last_hidden_state\n",
        "        bert_pooled_output = bert_output.pooler_output\n",
        "\n",
        "        # Project the hidden states into the style embedding space\n",
        "        style_embedding = self.style_encoder(bert_hidden_states).mean(dim=1)\n",
        "\n",
        "        # Decode the style embedding back into the original hidden state space\n",
        "        reconstructed_hidden_states = self.style_decoder(style_embedding.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Feed the reconstructed hidden states through GPT-2 to generate text\n",
        "        gpt_input = reconstructed_hidden_states\n",
        "        gpt_output = self.gpt_model(inputs_embeds=gpt_input)\n",
        "\n",
        "        return gpt_output.logits, style_embedding\n",
        "\n",
        "    def generate(self, content_inputs, content_attention_mask, style_vector):\n",
        "        with torch.no_grad():\n",
        "            logits = self.forward(content_inputs, content_attention_mask, style_vector=style_vector)\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            return self.gpt_model.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def style_transfer_loss(generated_style, target_style, generated_logits, target_logits, lambda_style=1.0, lambda_logits=1.0):\n",
        "    style_loss = lambda_style * nn.functional.mse_loss(generated_style, target_style)\n",
        "    logits_loss = lambda_logits * nn.functional.mse_loss(generated_logits, target_logits)\n",
        "    return style_loss + logits_loss"
      ],
      "metadata": {
        "id": "Gr1bmxyD2xo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_text = \"I love going to the beach on weekends.\"\n",
        "style_vector = torch.tensor(style_embeddings[-1]).mean(dim=0).unsqueeze(0).to(device) # replace with a learned style vector from the training data\n",
        "tokenized_source = bert_tokenizer(source_text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
        "content_inputs = tokenized_source['input_ids'].to(device)\n",
        "content_attention_mask = tokenized_source['attention_mask'].to(device)\n",
        "generated_text = model.generate(content_inputs, content_attention_mask, style_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "f3NQDxCDdkVy",
        "outputId": "6a4bf793-a1be-4209-bee9-7a6db6f58584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-7f6f588ec5a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcontent_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_source\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontent_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_source\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-9b269f3b7c03>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, content_inputs, content_attention_mask, style_vector)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9b269f3b7c03>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, content_inputs, content_attention_mask, style_inputs, style_attention_mask, style_vector)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mfused_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mfused_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mgpt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfused_latent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# repeat the tensor along the sequence dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mgpt_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1024 and 512x768)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(train_loader):\n",
        "    _content_inputs = data['content_input']['input_ids'].squeeze().to(device)\n",
        "    _content_attention_mask = data['content_input']['attention_mask'].squeeze().to(device)\n",
        "    _style_inputs = data['style_input']['input_ids'].squeeze().to(device)\n",
        "    _style_attention_mask = data['style_input']['attention_mask'].squeeze().to(device)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "c5v7Mo6X1-Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_content_inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj1oyEkuAdka",
        "outputId": "8469dacb-3433-4fd1-c707-b8079f5cfc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens = train_dataset.tokenizer('Why dont you fuck off', max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
        "style_tokens = train_dataset.tokenizer('You should leave', max_length=128, padding='max_length', truncation=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "EXjbGi1jnpRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(input_tokens['input_ids'].to(device), \n",
        "      input_tokens['attention_mask'].to(device), \n",
        "      style_tokens['input_ids'].to(device), \n",
        "      style_tokens['attention_mask'].to(device))"
      ],
      "metadata": {
        "id": "zQLRkinAB5HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_tokens = logits.argmax(dim=-1)\n",
        "predicted_text = train_dataset.tokenizer.decode(predicted_tokens, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "uphM0KoTMrZr",
        "outputId": "a5a9e5ea-4112-47ca-eeca-ab7e5657c273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-8aa9146b81a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredicted_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3469\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3471\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3472\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3473\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_use_source_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_source_tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;31m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.tokenizer.batch_decode(predicted_tokens, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5589IpuMrRJ",
        "outputId": "4ac4edfe-64e0-4f18-c079-0e279cebf7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"you '.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.tokenizer.batch_decode(model.gpt_model.generate(test_tokens['input_ids'].to(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5eXXTjHBZNm",
        "outputId": "5550fedc-7811-4147-ac61-816cda3cea35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Input length of input_ids is 128, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] shush you retarded dumbass [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4AIhJ1u1P7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVaL1BN3xh7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUV0O2uMvEld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}